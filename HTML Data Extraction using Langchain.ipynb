{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb672366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df24605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5af9a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.html.UnstructuredHTMLLoader at 0x1c372b0ecd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredHTMLLoader(file_path='transformers.html')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a7892b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Transformers\\n\\nOr as I like to call it Attention on Steroids. 💉💊\\n\\nRia Kulshrestha·Follow\\n\\nPublished inTowards Data Science·10 min read·Jun 29, 2020\\n\\nListenShareMore\\n\\n4\\n\\n-\\n\\nListen\\n\\nShare\\n\\nMore\\n\\nArseny Togulev on\\n\\nUnsplash\\n\\nNo, this article is not about the American sci-fi action movie series — no Optimus Prime here. It is also not about the electrical device that is used to transfer energy from one electrical circuit to another. What is this about then, you ask?\\n\\nIt is about the one in the most sci-fi fields of all time, Artificial Intelligence — Natural Language Processing in particular and it is pretty optimal at transferring information and primely used. (See what I did there. :P)\\n\\nThis post is based on the paper: Attention is All You Need. P.S. the authors were not kidding when they chose that title because you will need all the attention at your disposal for this one. But don’t let that scare you, it is SO SO worth it!!\\n\\nWhat is a Transformer?\\n\\nThe Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. It relies entirely on self-attention to compute representations of its input and output WITHOUT using sequence-aligned RNNs or convolution. 🤯\\n\\nIf you recall my previous post, Understanding Attention In Deep Learning, we discussed how and why many models fell short when it came to handling long-range dependencies. The concept of attention somewhat allowed us to overcome that problem and now in Transformers we will build on top of attention and unleash its full potential.\\n\\nUnderstanding Attention In Deep LearningHow a little attention changed the AI game!towardsdatascience.com\\n\\nFew things to know before diving into Transformers\\n\\nSelf-Attention\\n\\nLet us start with revisiting what attention is in the NLP universe? Understanding Attention In Deep Learning. (I apologize for these blatant self-advertisements, but seriously give it a read. It will help you under Transformers much better. I promise.)\\n\\nAttention allowed us to focus on parts of our input sequence while we predicted our output sequence. If our model predicted the word “rouge” [French translation for the color red], we are very likely to find a high weight-age for the word “red” in our input sequence. So attention, in a way, allowed us to map some connection/correlation between the input word “rouge” and the output word “red”.\\n\\nSelf attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\\n\\nIn simpler terms, self attention helps us create similar connections but within the same sentence. Look at the following example:\\n\\n“I poured water from the bottle into the cup until it was full.”it => cup\\n\\n“I poured water from the bottle into the cup until it was empty.”it=> bottle\\n\\nBy changing one word “full” — > “empty” the reference object for “it” changed. If we are translating such a sentence, we will want to know the word “it” refers to.\\n\\nThe three kinds of Attention possible in a model:\\n\\nEncoder-Decoder Attention: Attention between the input sequence and the output sequence.\\n\\nSelf attention in the input sequence: Attends to all the words in the input sequence.\\n\\nSelf attention in the output sequence: One thing we should be wary of here is that the scope of self attention is limited to the words that occur before a given word. This prevents any information leaks during the training of the model. This is done by masking the words that occur after it for each step. So for step 1, only the first word of the output sequence is NOT masked, for step 2, the first two words are NOT masked and so on.\\n\\nKeys, Values, and Queries:\\n\\nThe three random words I just threw at you in this heading are vectors created as abstractions are useful for calculating self attention, more details on each below. These are calculated by multiplying your input vector(X) with weight matrices that are learnt while training.\\n\\nQuery Vector: q= X * Wq. Think of this as the current word.\\n\\nKey Vector: k= X * Wk. Think of this as an indexing mechanism for Value vector. Similar to how we have key-value pairs in hash maps, where keys are used to uniquely index the values.\\n\\nValue Vector: v= X * Wv. Think of this as the information in the input word.\\n\\nAll these matrices Wq, Wk and Wv are learnt while being jointly trained during the model training.\\n\\nCalculating Self attention from q, k and v:\\n\\nIf we are calculating self attention for #i input word,\\n\\nStep 1: Multiply qᵢ by the kⱼ key vector of word.\\n\\nStep 2: Then divide this product by the square root of the dimension of key vector. This step is done for better gradient flow which is specially important in cases when the value of the dot product in previous step is too big. As using them directly might push the softmax into regions with very little gradient flow.\\n\\nStep 3: Once we have scores for all js, we pass these through a softmax. We get normalized value for each j.\\n\\nStep 4: Multiply softmax scores for each j with vᵢ vector. The idea/purpose here is, very similar attention, to keep preserve only the values v of the input word(s) we want to focus on by multiplying them with high probability scores from softmax ~1, and remove the rest by driving them towards 0, i.e. making them very small by multiplying them with the low probability scores ~0 from softmax.\\n\\nThe Transformer\\n\\n⚠️ A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2)\\n\\nBeast #1: Encoder-Decoder stacks\\n\\nEach encoder has two sub-layers.\\n\\nA multi-head self attention mechanism on the input vectors (Think parallelized and efficient sibling of self attention).\\n\\nA simple, position-wise fully connected feed-forward network (Think post-processing).\\n\\nCheck out this absolute bomb 3D diagram of the Encoder block used in BERT. Seriously you can’t miss this!!! It is like a whole new level of understanding.\\n\\nDecoder: Given z, the decoder then generates an output sequence (y₁, …, yₘ) of symbols one element at a time.\\n\\nEach decoder has three sub-layers.\\n\\nA masked multi-head self attention mechanism on the output vectors of the previous iteration.\\n\\nA multi-head attention mechanism on the output from encoder and masked multi-headed attention in decoder.\\n\\nA simple, position-wise fully connected feed-forward network (think post-processing).\\n\\nA few additional points:\\n\\nIn the original paper, 6 layers were present in the encoder stack (2 sub-layer version) and 6 in the decoder stack (3 sub-layer version).\\n\\nAll sub-layers in the model, as well as the embedding layers, produce outputs of the same dimension. This is done to facilitate the residual connections.\\n\\nBeast #2 Inside Encoder-Decoder stacks — Multi-Head Attention:\\n\\nWe just noted that the output of each sub-layer needs to be of the same dimension which is 512 in our paper.=> zᵢ needs to be of 512 dimensions. => vᵢ needs to be of 512 dimensions as zᵢ are just sort of weighted sums of vᵢs.\\n\\nAdditionally, we want to allow the model to focus on different positions is by calculating self attention multiple times with different sets of q, k and v vectors, then take an average of all those outputs to get our final z.\\n\\nThe size of the concatenated vector will be too large to be fed to the next sub-layer, so we scale it down by multiplying it with another learnt matrix Wo.\\n\\nMultiple attention heads allowed the model to jointly attend to information from different representation sub-spaces at different positions which was inhibited by averaging in a single attention head.\\n\\nBeast #3— Input and Output Pre-processing:\\n\\nThe input words are represented using some form of embedding. This is done for both encoder and decoder.\\n\\nWord embedding on their own lack any positional information which is achieved in RNNs by virtue of their sequential nature. Meanwhile in self-attention, due to softmax, any such positional information is lost.\\n\\nTo preserve the positional information, the transformer injects a vector to individual input embeddings (could be using word embeddings for corresponding to the input words). These vectors follow a specific periodic function (Example: combination of various sines/cosines having different frequency, in short not in sync with each other) that the model learns and is able to determine the position of individual word wrt each other based on the values .\\n\\nThis injected vector is called “positional encoding” and are added to the input embeddings at the bottoms of both encoder and decoder stacks.\\n\\nBeast #4 — Decoder stack: Revisited\\n\\nThe output of the decoder stack at each step is fed back to the decoder in the next time step — pretty similar to how outputs from previous steps in RNNs were used as next hidden states. And just as we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to preserve the position of each word. This positional encoding + word embedding combo is then fed into a masked multi-headed self attention.\\n\\nThis self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions — you can’t look at future words. This masking ensures that the predictions for position i can depend only on the known outputs at positions less than i.\\n\\nThe outputs from the encoder stack are then used as multiple sets of key vectors k and value vectors v, for the “encoder decoder attention” — shown in green in the diagram — layer. It helps the decoder focus on the contextually relevant parts in the input sequence for that step. (The part similar to global attention vectors.) The q vector comes from the “output self attention” layer.\\n\\nOnce we get the output from the decoder, we do a softmax again to select the final probabilities of words.\\n\\nConclusion\\n\\nLet’s finish with a quick wrap-up revision.\\n\\nWe started with understanding what self attention is and how to calculate self-attention from these v, k ,q vectors.\\n\\nMulti-headed attention is an efficient modification of self attention that uses multiple smaller sets of v, k ,q and concatenates the outputs from each set to get the final z.\\n\\nThen we saw how and where the three kinds of self attention are used in the model.\\n\\nFollowed by the pre-processing done on the inputs for the encoder and decoder stacks.\\n\\nReferences + Recommended Reads\\n\\nUnderstanding Deep Attention in Deep Learning if you faced issues around attention.\\n\\nThe Illustrated Transformer — Has great visualizations along with the explanations.\\n\\nhttps://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/\\n\\nYouTube video #1: A great resource to get the model intuition. Specially if you want to learn more about positional encoding.\\n\\nYouTube video#2: Talk by Lukasz Kaiser on this paper explaining self attention — . P.S. He is one of the authors for this paper.\\n\\nI’m glad you made it till the end of this article. 🎉I hope your reading experience was as enriching as the one I had writing this. 💖\\n\\nIf you liked this article, do check out my other ones here.\\n\\nIf you want to reach out to me, my medium of choice would be Twitter.', metadata={'source': 'transformers.html'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63c0b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transformers.html'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c34c254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers\n",
      "\n",
      "Or as I like to call it Attention on Steroids. 💉💊\n",
      "\n",
      "Ria Kulshrestha·Follow\n",
      "\n",
      "Published inTowards Data Science·10 min read·Jun 29, 2020\n",
      "\n",
      "ListenShareMore\n",
      "\n",
      "4\n",
      "\n",
      "-\n",
      "\n",
      "Listen\n",
      "\n",
      "Share\n",
      "\n",
      "More\n",
      "\n",
      "Arseny Togulev on\n",
      "\n",
      "Unsplash\n",
      "\n",
      "No, this article is not about the American sci-fi action movie series — no Optimus Prime here. It is also not about the electrical device that is used to transfer energy from one electrical circuit to another. What is this about then, you ask?\n",
      "\n",
      "It is about the one in the most sci-fi fields of all time, Artificial Intelligence — Natural Language Processing in particular and it is pretty optimal at transferring information and primely used. (See what I did there. :P)\n",
      "\n",
      "This post is based on the paper: Attention is All You Need. P.S. the authors were not kidding when they chose that title because you will need all the attention at your disposal for this one. But don’t let that scare you, it is SO SO worth it!!\n",
      "\n",
      "What is a Transformer?\n",
      "\n",
      "The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. It relies entirely on self-attention to compute representations of its input and output WITHOUT using sequence-aligned RNNs or convolution. 🤯\n",
      "\n",
      "If you recall my previous post, Understanding Attention In Deep Learning, we discussed how and why many models fell short when it came to handling long-range dependencies. The concept of attention somewhat allowed us to overcome that problem and now in Transformers we will build on top of attention and unleash its full potential.\n",
      "\n",
      "Understanding Attention In Deep LearningHow a little attention changed the AI game!towardsdatascience.com\n",
      "\n",
      "Few things to know before diving into Transformers\n",
      "\n",
      "Self-Attention\n",
      "\n",
      "Let us start with revisiting what attention is in the NLP universe? Understanding Attention In Deep Learning. (I apologize for these blatant self-advertisements, but seriously give it a read. It will help you under Transformers much better. I promise.)\n",
      "\n",
      "Attention allowed us to focus on parts of our input sequence while we predicted our output sequence. If our model predicted the word “rouge” [French translation for the color red], we are very likely to find a high weight-age for the word “red” in our input sequence. So attention, in a way, allowed us to map some connection/correlation between the input word “rouge” and the output word “red”.\n",
      "\n",
      "Self attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
      "\n",
      "In simpler terms, self attention helps us create similar connections but within the same sentence. Look at the following example:\n",
      "\n",
      "“I poured water from the bottle into the cup until it was full.”it => cup\n",
      "\n",
      "“I poured water from the bottle into the cup until it was empty.”it=> bottle\n",
      "\n",
      "By changing one word “full” — > “empty” the reference object for “it” changed. If we are translating such a sentence, we will want to know the word “it” refers to.\n",
      "\n",
      "The three kinds of Attention possible in a model:\n",
      "\n",
      "Encoder-Decoder Attention: Attention between the input sequence and the output sequence.\n",
      "\n",
      "Self attention in the input sequence: Attends to all the words in the input sequence.\n",
      "\n",
      "Self attention in the output sequence: One thing we should be wary of here is that the scope of self attention is limited to the words that occur before a given word. This prevents any information leaks during the training of the model. This is done by masking the words that occur after it for each step. So for step 1, only the first word of the output sequence is NOT masked, for step 2, the first two words are NOT masked and so on.\n",
      "\n",
      "Keys, Values, and Queries:\n",
      "\n",
      "The three random words I just threw at you in this heading are vectors created as abstractions are useful for calculating self attention, more details on each below. These are calculated by multiplying your input vector(X) with weight matrices that are learnt while training.\n",
      "\n",
      "Query Vector: q= X * Wq. Think of this as the current word.\n",
      "\n",
      "Key Vector: k= X * Wk. Think of this as an indexing mechanism for Value vector. Similar to how we have key-value pairs in hash maps, where keys are used to uniquely index the values.\n",
      "\n",
      "Value Vector: v= X * Wv. Think of this as the information in the input word.\n",
      "\n",
      "All these matrices Wq, Wk and Wv are learnt while being jointly trained during the model training.\n",
      "\n",
      "Calculating Self attention from q, k and v:\n",
      "\n",
      "If we are calculating self attention for #i input word,\n",
      "\n",
      "Step 1: Multiply qᵢ by the kⱼ key vector of word.\n",
      "\n",
      "Step 2: Then divide this product by the square root of the dimension of key vector. This step is done for better gradient flow which is specially important in cases when the value of the dot product in previous step is too big. As using them directly might push the softmax into regions with very little gradient flow.\n",
      "\n",
      "Step 3: Once we have scores for all js, we pass these through a softmax. We get normalized value for each j.\n",
      "\n",
      "Step 4: Multiply softmax scores for each j with vᵢ vector. The idea/purpose here is, very similar attention, to keep preserve only the values v of the input word(s) we want to focus on by multiplying them with high probability scores from softmax ~1, and remove the rest by driving them towards 0, i.e. making them very small by multiplying them with the low probability scores ~0 from softmax.\n",
      "\n",
      "The Transformer\n",
      "\n",
      "⚠️ A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2)\n",
      "\n",
      "Beast #1: Encoder-Decoder stacks\n",
      "\n",
      "Each encoder has two sub-layers.\n",
      "\n",
      "A multi-head self attention mechanism on the input vectors (Think parallelized and efficient sibling of self attention).\n",
      "\n",
      "A simple, position-wise fully connected feed-forward network (Think post-processing).\n",
      "\n",
      "Check out this absolute bomb 3D diagram of the Encoder block used in BERT. Seriously you can’t miss this!!! It is like a whole new level of understanding.\n",
      "\n",
      "Decoder: Given z, the decoder then generates an output sequence (y₁, …, yₘ) of symbols one element at a time.\n",
      "\n",
      "Each decoder has three sub-layers.\n",
      "\n",
      "A masked multi-head self attention mechanism on the output vectors of the previous iteration.\n",
      "\n",
      "A multi-head attention mechanism on the output from encoder and masked multi-headed attention in decoder.\n",
      "\n",
      "A simple, position-wise fully connected feed-forward network (think post-processing).\n",
      "\n",
      "A few additional points:\n",
      "\n",
      "In the original paper, 6 layers were present in the encoder stack (2 sub-layer version) and 6 in the decoder stack (3 sub-layer version).\n",
      "\n",
      "All sub-layers in the model, as well as the embedding layers, produce outputs of the same dimension. This is done to facilitate the residual connections.\n",
      "\n",
      "Beast #2 Inside Encoder-Decoder stacks — Multi-Head Attention:\n",
      "\n",
      "We just noted that the output of each sub-layer needs to be of the same dimension which is 512 in our paper.=> zᵢ needs to be of 512 dimensions. => vᵢ needs to be of 512 dimensions as zᵢ are just sort of weighted sums of vᵢs.\n",
      "\n",
      "Additionally, we want to allow the model to focus on different positions is by calculating self attention multiple times with different sets of q, k and v vectors, then take an average of all those outputs to get our final z.\n",
      "\n",
      "The size of the concatenated vector will be too large to be fed to the next sub-layer, so we scale it down by multiplying it with another learnt matrix Wo.\n",
      "\n",
      "Multiple attention heads allowed the model to jointly attend to information from different representation sub-spaces at different positions which was inhibited by averaging in a single attention head.\n",
      "\n",
      "Beast #3— Input and Output Pre-processing:\n",
      "\n",
      "The input words are represented using some form of embedding. This is done for both encoder and decoder.\n",
      "\n",
      "Word embedding on their own lack any positional information which is achieved in RNNs by virtue of their sequential nature. Meanwhile in self-attention, due to softmax, any such positional information is lost.\n",
      "\n",
      "To preserve the positional information, the transformer injects a vector to individual input embeddings (could be using word embeddings for corresponding to the input words). These vectors follow a specific periodic function (Example: combination of various sines/cosines having different frequency, in short not in sync with each other) that the model learns and is able to determine the position of individual word wrt each other based on the values .\n",
      "\n",
      "This injected vector is called “positional encoding” and are added to the input embeddings at the bottoms of both encoder and decoder stacks.\n",
      "\n",
      "Beast #4 — Decoder stack: Revisited\n",
      "\n",
      "The output of the decoder stack at each step is fed back to the decoder in the next time step — pretty similar to how outputs from previous steps in RNNs were used as next hidden states. And just as we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to preserve the position of each word. This positional encoding + word embedding combo is then fed into a masked multi-headed self attention.\n",
      "\n",
      "This self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions — you can’t look at future words. This masking ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "\n",
      "The outputs from the encoder stack are then used as multiple sets of key vectors k and value vectors v, for the “encoder decoder attention” — shown in green in the diagram — layer. It helps the decoder focus on the contextually relevant parts in the input sequence for that step. (The part similar to global attention vectors.) The q vector comes from the “output self attention” layer.\n",
      "\n",
      "Once we get the output from the decoder, we do a softmax again to select the final probabilities of words.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "Let’s finish with a quick wrap-up revision.\n",
      "\n",
      "We started with understanding what self attention is and how to calculate self-attention from these v, k ,q vectors.\n",
      "\n",
      "Multi-headed attention is an efficient modification of self attention that uses multiple smaller sets of v, k ,q and concatenates the outputs from each set to get the final z.\n",
      "\n",
      "Then we saw how and where the three kinds of self attention are used in the model.\n",
      "\n",
      "Followed by the pre-processing done on the inputs for the encoder and decoder stacks.\n",
      "\n",
      "References + Recommended Reads\n",
      "\n",
      "Understanding Deep Attention in Deep Learning if you faced issues around attention.\n",
      "\n",
      "The Illustrated Transformer — Has great visualizations along with the explanations.\n",
      "\n",
      "https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/\n",
      "\n",
      "YouTube video #1: A great resource to get the model intuition. Specially if you want to learn more about positional encoding.\n",
      "\n",
      "YouTube video#2: Talk by Lukasz Kaiser on this paper explaining self attention — . P.S. He is one of the authors for this paper.\n",
      "\n",
      "I’m glad you made it till the end of this article. 🎉I hope your reading experience was as enriching as the one I had writing this. 💖\n",
      "\n",
      "If you liked this article, do check out my other ones here.\n",
      "\n",
      "If you want to reach out to me, my medium of choice would be Twitter.\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
