<!doctype html><html lang="en"><head><title data-rh="true">Transformers in NLP: A beginner friendly explanation | Towards Data Science</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2020-11-22T11:01:46.716Z"/><meta data-rh="true" name="title" content="Transformers in NLP: A beginner friendly explanation | Towards Data Science"/><meta data-rh="true" property="og:title" content="Transformers"/><meta data-rh="true" property="al:android:url" content="medium://p/89034557de14"/><meta data-rh="true" property="al:ios:url" content="medium://p/89034557de14"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease using self attention."/><meta data-rh="true" property="og:description" content="Or as I like to call it Attention on Steroids. 💉💊"/><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/transformers-89034557de14"/><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/transformers-89034557de14"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/v2/da:true/resize:fit:1200/0*2YfRrpQtt8TG2iuz"/><meta data-rh="true" property="og:image:alt" content="A picture of Transformer from the movie franchise."/><meta data-rh="true" property="article:author" content="https://ria-kulshrestha.medium.com"/><meta data-rh="true" name="author" content="Ria Kulshrestha"/><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" property="twitter:title" content="Transformers"/><meta data-rh="true" name="twitter:site" content="@TDataScience"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/89034557de14"/><meta data-rh="true" property="twitter:description" content="Or as I like to call it Attention on Steroids. 💉💊"/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/v2/da:true/resize:fit:1200/0*2YfRrpQtt8TG2iuz"/><meta data-rh="true" name="twitter:image:alt" content="A picture of Transformer from the movie franchise."/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="10 min read"/><link data-rh="true" rel="icon" href="https://miro.medium.com/v2/resize:fill:256:256/1*VzTUkfeGymHP4Bvav-T-lA.png"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/v2/resize:fill:152:152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/v2/resize:fill:120:120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/v2/resize:fill:76:76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/v2/resize:fill:60:60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://ria-kulshrestha.medium.com"/><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/transformers-89034557de14"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/89034557de14"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fda:true\u002Fresize:fit:1200\u002F0*2YfRrpQtt8TG2iuz"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Ftransformers-89034557de14","dateCreated":"2020-06-29T08:44:31.505Z","datePublished":"2020-06-29T08:44:31.505Z","dateModified":"2021-12-15T01:52:18.845Z","headline":"Transformers in NLP: A beginner friendly explanation | Towards Data Science","name":"Transformers in NLP: A beginner friendly explanation | Towards Data Science","description":"The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease using self attention.","identifier":"89034557de14","author":{"@type":"Person","name":"Ria Kulshrestha","url":"https:\u002F\u002Fria-kulshrestha.medium.com"},"creator":["Ria Kulshrestha"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":192,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:384\u002F1*cFFKn8rFH4ZndmaYeAs6iQ.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Ftransformers-89034557de14"}</script><style type="text/css" data-fela-rehydration="582" data-fela-type="STATIC">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden="true"]{visibility:hidden;pointer-events:none}.grecaptcha-badge{visibility:hidden}
/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;
}/* Gray DOCTYPE selectors like WebKit */
.xml .hljs-meta {color: #c0c0c0;
}.hljs-comment,
.hljs-quote {color: #007400;
}.hljs-tag,
.hljs-attribute,
.hljs-keyword,
.hljs-selector-tag,
.hljs-literal,
.hljs-name {color: #aa0d91;
}.hljs-variable,
.hljs-template-variable {color: #3F6E74;
}.hljs-code,
.hljs-string,
.hljs-meta .hljs-string {color: #c41a16;
}.hljs-regexp,
.hljs-link {color: #0E0EFF;
}.hljs-title,
.hljs-symbol,
.hljs-bullet,
.hljs-number {color: #1c00cf;
}.hljs-section,
.hljs-meta {color: #643820;
}.hljs-title.class_,
.hljs-class .hljs-title,
.hljs-type,
.hljs-built_in,
.hljs-params {color: #5c2699;
}.hljs-attr {color: #836C28;
}.hljs-subst {color: #000;
}.hljs-formula {background-color: #eee;font-style: italic;
}.hljs-addition {background-color: #baeeba;
}.hljs-deletion {background-color: #ffc8bd;
}.hljs-selector-id,
.hljs-selector-class {color: #9b703f;
}.hljs-doctag,
.hljs-strong {font-weight: bold;
}.hljs-emphasis {font-style: italic;
}
</style><style type="text/css" data-fela-rehydration="582" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-webkit-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au{fill:rgba(0, 0, 0, 1)}.av{height:22px}.aw{margin-left:16px}.ax{border:none}.ay{border-radius:20px}.az{width:240px}.ba{background:#F9F9F9}.bb path{fill:#6B6B6B}.bd{outline:none}.be{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bf{font-size:14px}.bg{width:100%}.bh{padding:10px 20px 10px 0}.bi{background-color:transparent}.bj{color:#242424}.bk::placeholder{color:#6B6B6B}.bl{display:inline-block}.bm{margin-left:12px}.bn{margin-right:12px}.bo{border-radius:4px}.bp{margin-left:24px}.bq{height:24px}.bw{background-color:#F9F9F9}.bx{border-radius:50%}.by{height:32px}.bz{width:32px}.ca{justify-content:center}.cg{max-width:680px}.ch{min-width:0}.ci{animation:k1 1.2s ease-in-out infinite}.cj{height:100vh}.ck{margin-bottom:16px}.cl{margin-top:48px}.cm{align-items:flex-start}.cn{flex-direction:column}.co{justify-content:space-between}.cp{margin-bottom:24px}.cv{width:80%}.cw{background-color:#F2F2F2}.dc{height:44px}.dd{width:44px}.de{margin:auto 0}.df{margin-bottom:4px}.dg{height:16px}.dh{width:120px}.di{width:80px}.do{margin-bottom:8px}.dp{width:96%}.dq{width:98%}.dr{width:81%}.dv{margin-left:8px}.dw{color:#6B6B6B}.dx{font-size:13px}.dy{height:100%}.dz{height:25px}.ea{fill:rgba(41, 41, 41, 1)}.ed{margin-right:32px}.ee{position:relative}.ef{fill:#6B6B6B}.ei{background:transparent}.ej svg{margin-left:4px}.ek svg{fill:#6B6B6B}.em{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.eo{position:absolute}.eq{box-sizing:border-box}.ew{margin:0 24px}.fa{background:rgba(255, 255, 255, 1)}.fb{border:1px solid #F2F2F2}.fc{box-shadow:0 1px 4px #F2F2F2}.fd{max-height:100vh}.fe{overflow-y:auto}.ff{left:0}.fg{top:calc(100vh + 100px)}.fh{bottom:calc(100vh + 100px)}.fi{width:10px}.fj{pointer-events:none}.fk{word-break:break-word}.fl{word-wrap:break-word}.fm:after{display:block}.fn:after{content:""}.fo:after{clear:both}.fp{line-height:1.23}.fq{letter-spacing:0}.fr{font-style:normal}.fs{font-weight:700}.gn{margin-bottom:-0.27em}.go{line-height:1.394}.he{@media all and (max-width: 551.98px):8px}.hf{@media all and (min-width: 552px) and (max-width: 727.98px):8px}.hg{@media all and (min-width: 728px) and (max-width: 903.98px):16px}.hh{@media all and (min-width: 904px) and (max-width: 1079.98px):16px}.hi{@media all and (min-width: 1080px):16px}.ho{align-items:baseline}.hp{width:48px}.hq{height:48px}.hr{border:2px solid rgba(255, 255, 255, 1)}.hs{z-index:0}.ht{box-shadow:none}.hu{border:1px solid rgba(0, 0, 0, 0.05)}.hv{margin-left:-12px}.hw{width:28px}.hx{height:28px}.hy{z-index:1}.hz{width:24px}.ia{margin-bottom:2px}.ib{flex-wrap:nowrap}.ic{font-size:16px}.id{line-height:24px}.if{margin:0 8px}.ig{display:inline}.ih{color:rgba(102, 138, 170, 1)}.ii{fill:rgba(102, 138, 170, 1)}.ij:disabled{opacity:0.3}.im{flex:0 0 auto}.ip{flex-wrap:wrap}.is{white-space:pre-wrap}.it{margin-right:4px}.iu{overflow:hidden}.iv{max-height:20px}.iw{text-overflow:ellipsis}.ix{display:-webkit-box}.iy{-webkit-line-clamp:1}.iz{-webkit-box-orient:vertical}.ja{word-break:break-all}.jc{padding-left:8px}.jd{padding-right:8px}.ke> *{flex-shrink:0}.kf{overflow-x:scroll}.kg::-webkit-scrollbar{display:none}.kh{scrollbar-width:none}.ki{-ms-overflow-style:none}.kj{width:74px}.kk{flex-direction:row}.kl{z-index:2}.ko{-webkit-user-select:none}.kp{border:0}.kq{cursor:progress}.kr{fill:rgba(117, 117, 117, 1)}.ku{opacity:0.25}.kv{outline:0}.kw{user-select:none}.kx> svg{pointer-events:none}.lg{margin-left:4px}.lh{margin-top:0px}.li{opacity:1}.lj{padding:4px 0}.lm{width:16px}.ln{padding:8px 2px}.lq svg path{fill:#6B6B6B}.lr path{fill:#242424}.ls{display:inline-flex}.ly{max-width:100%}.lz svg{color:#6B6B6B}.mq{margin-left:auto}.mr{margin-right:auto}.ms{max-width:7952px}.my{clear:both}.na{cursor:zoom-in}.nb{z-index:auto}.nd{height:auto}.ne{margin-top:10px}.nf{text-align:center}.ng{max-width:728px}.nj{text-decoration:underline}.nk{line-height:1.58}.nl{letter-spacing:-0.004em}.nm{font-family:source-serif-pro, Georgia, Cambria, "Times New Roman", Times, serif}.of{margin-bottom:-0.46em}.og{line-height:1.12}.oh{letter-spacing:-0.022em}.oi{font-weight:600}.pb{margin-bottom:-0.28em}.pm{box-shadow:inset 0 0 0 1px #F2F2F2}.pn{padding:0px}.po{padding:16px 20px}.pp{flex:1 1 auto}.pr{max-height:40px}.ps{-webkit-line-clamp:2}.pt{margin-top:8px}.pu{margin-top:12px}.pv{width:160px}.pw{background-image:url(https://miro.medium.com/v2/da:true/resize:fit:320/0*3WEpWAkyXjUHbNbG)}.px{background-origin:border-box}.py{background-size:cover}.pz{height:167px}.qa{background-position:50% 50%}.qb{line-height:1.18}.qr{margin-bottom:-0.31em}.qs{font-style:italic}.qt{box-shadow:inset 3px 0 0 0 #242424}.qu{padding-left:23px}.qv{margin-left:-20px}.qw{overflow-x:auto}.qx{font-family:source-code-pro, Menlo, Monaco, "Courier New", Courier, monospace}.qy{padding:20px}.qz{border-radius:0}.ra{background:#F2F2F2}.rb{margin-top:-0.09em}.rc{margin-bottom:-0.09em}.rd{min-width:fit-content}.re{margin-top:0.91em}.rf{list-style-type:decimal}.rg{margin-left:30px}.rh{padding-left:0px}.rn{list-style-type:disc}.ro{max-width:1168px}.rp{max-width:637px}.rq{max-width:743px}.rw{max-width:1640px}.rx{padding-top:5px}.ry{padding-bottom:5px}.rz{max-width:1251px}.sa{max-width:1212px}.sb{margin-top:32px}.sc{margin-bottom:14px}.sd{padding-top:24px}.se{padding-bottom:10px}.sf{background-color:#000000}.sg{height:3px}.sh{width:3px}.si{margin-right:20px}.so{margin:auto}.sp{padding-bottom:56.206088992974244%}.sq{height:0}.sr{margin-bottom:26px}.ss{margin-top:6px}.st{margin-right:8px}.su{padding:8px 16px}.sv{border-radius:100px}.sw{transition:background 300ms ease}.sy{white-space:nowrap}.sz{border-top:none}.tf{height:52px}.tg{max-height:52px}.th{box-sizing:content-box}.ti{position:static}.tk{max-width:155px}.tv{align-items:flex-end}.tw{width:76px}.tx{height:76px}.ty{border:2px solid #F9F9F9}.tz{height:72px}.ua{width:72px}.ub{margin-left:-16px}.uc{width:36px}.ud{height:36px}.ue{color:#F2F2F2}.uf{fill:#F2F2F2}.ug{border-color:#F2F2F2}.um:disabled{cursor:inherit !important}.un:disabled:hover{background:rgba(102, 138, 170, 1)}.uo:disabled:hover{border-color:rgba(102, 138, 170, 1)}.up{border-radius:99em}.uq{width:auto}.ur{border-width:1px}.us{border-style:solid}.ut{text-decoration:none}.uu{stroke:#F2F2F2}.uv{font-weight:500}.uw{font-size:24px}.ux{line-height:30px}.uy{letter-spacing:-0.016em}.uz{margin-top:16px}.va{height:0px}.vb{border-bottom:solid 1px #E5E5E5}.vh{margin-top:72px}.vi{padding:24px 0}.vj{margin-bottom:0px}.vk{margin-right:16px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.eg:hover{color:#242424}.eh:hover{fill:#242424}.el:hover svg{fill:#242424}.ep:hover{background-color:rgba(0, 0, 0, 0.1)}.ie:hover{text-decoration:underline}.ik:hover:not(:disabled){color:rgba(90, 118, 144, 1)}.il:hover:not(:disabled){fill:rgba(90, 118, 144, 1)}.kt:hover{fill:rgba(117, 117, 117, 1)}.lk:hover{fill:#000000}.ll:hover p{color:#000000}.lo:hover:not(:disabled) svg path{fill:#000000}.ma:hover svg{color:#000000}.sx:hover{background-color:#F2F2F2}.uh:hover{background:#F2F2F2}.ui:hover{border-color:#F2F2F2}.uj:hover{cursor:wait}.uk:hover{color:#F2F2F2}.ul:hover{fill:#F2F2F2}.bc:focus-within path{fill:#242424}.ks:focus{fill:rgba(117, 117, 117, 1)}.lp:focus svg path{fill:#000000}.mb:focus svg{color:#000000}.nc:focus{transform:scale(1.01)}.ky:active{border-style:none}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ec{display:flex}.ev{margin-bottom:68px}.ez{max-width:680px}.gj{font-size:42px}.gk{margin-top:1.19em}.gl{line-height:52px}.gm{letter-spacing:-0.011em}.hb{font-size:22px}.hc{margin-top:0.92em}.hd{line-height:28px}.hn{align-items:center}.jq{border-top:solid 1px #F2F2F2}.jr{border-bottom:solid 1px #F2F2F2}.js{margin:32px 0 0}.jt{padding:3px 8px}.kc> *{margin-right:24px}.kd> :last-child{margin-right:0}.lf{margin-top:0px}.lx{margin:0}.mx{margin-top:56px}.ob{font-size:20px}.oc{margin-top:2.14em}.od{line-height:32px}.oe{letter-spacing:-0.003em}.ox{font-size:24px}.oy{margin-top:1.95em}.oz{line-height:30px}.pa{letter-spacing:-0.016em}.pg{margin-top:0.94em}.pl{margin-top:32px}.qo{margin-top:1.72em}.qp{line-height:24px}.qq{letter-spacing:0}.rm{margin-top:1.14em}.rv{max-width:1192px}.sn{margin-top:1.25em}.te{margin-bottom:88px}.tp{display:inline-block}.tu{padding-top:72px}.vg{margin-top:40px}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.le{margin-top:0px}.nh{margin-left:auto}.ni{text-align:center}.to{display:inline-block}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.ld{margin-top:0px}.tn{display:inline-block}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.lb{margin-top:0px}.lc{margin-right:0px}.pq{padding:10px 12px 10px}.tm{display:inline-block}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.s{display:flex}.t{justify-content:space-between}.br{width:24px}.cb{margin:0 24px}.cq{height:40px}.cx{margin-bottom:44px}.dj{margin-bottom:32px}.ds{justify-content:center}.er{margin-bottom:4px}.ft{font-size:32px}.fu{margin-top:1.01em}.fv{line-height:38px}.fw{letter-spacing:-0.014em}.gp{font-size:18px}.gq{margin-top:0.79em}.gr{line-height:24px}.hj{align-items:flex-start}.in{flex-direction:column}.iq{margin-bottom:2px}.je{margin:24px -24px 0}.jf{padding:0}.ju> *{margin-right:8px}.jv> :last-child{margin-right:24px}.km{margin-left:0px}.kz{margin-top:0px}.la{margin-right:0px}.lt{margin:0}.mc{border:1px solid #F2F2F2}.md{border-radius:99em}.me{padding:0px 16px 0px 12px}.mf{height:38px}.mg{align-items:center}.mi svg{margin-right:8px}.mt{margin-top:40px}.nn{margin-top:1.56em}.no{line-height:28px}.np{letter-spacing:-0.003em}.oj{font-size:20px}.ok{margin-top:1.2em}.ol{letter-spacing:0}.pc{margin-top:0.67em}.ph{margin-top:24px}.qc{font-size:16px}.qd{margin-top:1.23em}.qe{line-height:20px}.ri{margin-top:1.34em}.rr{max-width:100%}.sj{margin-top:0.93em}.ta{margin-bottom:80px}.tl{display:inline-block}.tq{padding-top:48px}.vc{margin-top:32px}.mh:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.bu{width:64px}.ce{margin:0 64px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.eb{display:flex}.eu{margin-bottom:68px}.ey{max-width:680px}.gf{font-size:42px}.gg{margin-top:1.19em}.gh{line-height:52px}.gi{letter-spacing:-0.011em}.gy{font-size:22px}.gz{margin-top:0.92em}.ha{line-height:28px}.hm{align-items:center}.jm{border-top:solid 1px #F2F2F2}.jn{border-bottom:solid 1px #F2F2F2}.jo{margin:32px 0 0}.jp{padding:3px 8px}.ka> *{margin-right:24px}.kb> :last-child{margin-right:0}.lw{margin:0}.mw{margin-top:56px}.nx{font-size:20px}.ny{margin-top:2.14em}.nz{line-height:32px}.oa{letter-spacing:-0.003em}.ot{font-size:24px}.ou{margin-top:1.95em}.ov{line-height:30px}.ow{letter-spacing:-0.016em}.pf{margin-top:0.94em}.pk{margin-top:32px}.ql{margin-top:1.72em}.qm{line-height:24px}.qn{letter-spacing:0}.rl{margin-top:1.14em}.ru{max-width:1192px}.sm{margin-top:1.25em}.td{margin-bottom:88px}.tt{padding-top:72px}.vf{margin-top:40px}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.w{display:flex}.x{justify-content:space-between}.bt{width:64px}.cd{margin:0 48px}.cs{height:48px}.cz{margin-bottom:52px}.dl{margin-bottom:48px}.du{justify-content:center}.et{margin-bottom:68px}.ex{max-width:680px}.gb{font-size:42px}.gc{margin-top:1.19em}.gd{line-height:52px}.ge{letter-spacing:-0.011em}.gv{font-size:22px}.gw{margin-top:0.92em}.gx{line-height:28px}.hl{align-items:center}.ji{border-top:solid 1px #F2F2F2}.jj{border-bottom:solid 1px #F2F2F2}.jk{margin:32px 0 0}.jl{padding:3px 8px}.jy> *{margin-right:24px}.jz> :last-child{margin-right:0}.lv{margin:0}.mv{margin-top:56px}.nt{font-size:20px}.nu{margin-top:2.14em}.nv{line-height:32px}.nw{letter-spacing:-0.003em}.op{font-size:24px}.oq{margin-top:1.95em}.or{line-height:30px}.os{letter-spacing:-0.016em}.pe{margin-top:0.94em}.pj{margin-top:32px}.qi{margin-top:1.72em}.qj{line-height:24px}.qk{letter-spacing:0}.rk{margin-top:1.14em}.rt{max-width:100%}.sl{margin-top:1.25em}.tc{margin-bottom:88px}.ts{padding-top:72px}.ve{margin-top:40px}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dt{justify-content:center}.es{margin-bottom:4px}.fx{font-size:32px}.fy{margin-top:1.01em}.fz{line-height:38px}.ga{letter-spacing:-0.014em}.gs{font-size:18px}.gt{margin-top:0.79em}.gu{line-height:24px}.hk{align-items:flex-start}.io{flex-direction:column}.ir{margin-bottom:2px}.jg{margin:24px 0 0}.jh{padding:0}.jw> *{margin-right:8px}.jx> :last-child{margin-right:8px}.kn{margin-left:0px}.lu{margin:0}.mj{border:1px solid #F2F2F2}.mk{border-radius:99em}.ml{padding:0px 16px 0px 12px}.mm{height:38px}.mn{align-items:center}.mp svg{margin-right:8px}.mu{margin-top:40px}.nq{margin-top:1.56em}.nr{line-height:28px}.ns{letter-spacing:-0.003em}.om{font-size:20px}.on{margin-top:1.2em}.oo{letter-spacing:0}.pd{margin-top:0.67em}.pi{margin-top:24px}.qf{font-size:16px}.qg{margin-top:1.23em}.qh{line-height:20px}.rj{margin-top:1.34em}.rs{max-width:100%}.sk{margin-top:0.93em}.tb{margin-bottom:80px}.tr{padding-top:48px}.vd{margin-top:32px}.mo:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE" media="print">.tj{display:none}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.jb{max-height:none}</style><style type="text/css" data-fela-rehydration="582" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.mz{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="l c"><div class="l m n o c"><div class="am q r s ds u dt w du i d y z"><a class="dw ag dx be ak b am an ao ap aq ar as at s u w i d q dy z" href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F89034557de14&amp;%7Efeature=LiOpenInAppButton&amp;%7Echannel=ShowPostUnderCollection&amp;source=---two_column_layout_nav----------------------------------" rel="noopener follow">Open in app<svg width="10" height="10" viewBox="0 0 10 10" fill="none" class="dv"><path d="M.98 8.48a.37.37 0 1 0 .54.54l-.54-.54zm7.77-7.23h.38c0-.2-.17-.38-.38-.38v.38zM8.37 6.5a.37.37 0 1 0 .76 0h-.76zM3.5.87a.37.37 0 1 0 0 .76V.88zM1.52 9.03l7.5-7.5-.54-.54-7.5 7.5.54.54zm6.86-7.77V6.5h.74V1.25h-.74zm-4.88.38h5.25V.88H3.5v.74z" fill="currentColor"></path></svg></a></div><div class="p q r ab ac"><div class="ab q ae"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab" aria-label="Homepage" data-testid="headerMediumLogo" href="https://medium.com/?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><svg viewBox="0 0 1043.63 592.71" class="dz ea"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a><div class="aw h"><div class="ab ax ay az ba q bb bc"><div class="bl" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"></div><div class="bm bn ab"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z" fill="currentColor"></path></svg></div><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" data-testid="headerSearchInput" tabindex="0" class="ax bd be bf z bg bh bi bj bk" placeholder="Search" value=""/></div></div></div><div class="h k w eb ec"><div class="ed ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerWriteButton" href="https://medium.com/new-story?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><div class="be b bf z dw ee ef ab q eg eh"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Write"><path d="M14 4a.5.5 0 0 0 0-1v1zm7 6a.5.5 0 0 0-1 0h1zm-7-7H4v1h10V3zM3 4v16h1V4H3zm1 17h16v-1H4v1zm17-1V10h-1v10h1zm-1 1a1 1 0 0 0 1-1h-1v1zM3 20a1 1 0 0 0 1 1v-1H3zM4 3a1 1 0 0 0-1 1h1V3z" fill="currentColor"></path><path d="M17.5 4.5l-8.46 8.46a.25.25 0 0 0-.06.1l-.82 2.47c-.07.2.12.38.31.31l2.47-.82a.25.25 0 0 0 .1-.06L19.5 6.5m-2-2l2.32-2.32c.1-.1.26-.1.36 0l1.64 1.64c.1.1.1.26 0 .36L19.5 6.5m-2-2l2 2" stroke="currentColor"></path></svg><div class="dv l">Write</div></div></a></div></div><div class="k j i d"><div class="ed ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSearchButton" href="https://medium.com/search?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><div class="be b bf z dw ee ef ab q eg eh"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Search"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z" fill="currentColor"></path></svg></div></a></div></div><div class="ed ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerNotificationButton" href="https://medium.com/me/notifications?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><div class="be b bf z dw ee ef ab q eg eh"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Notifications"><path d="M15 18.5a3 3 0 1 1-6 0" stroke="currentColor" stroke-linecap="round"></path><path d="M5.5 10.53V9a6.5 6.5 0 0 1 13 0v1.53c0 1.42.56 2.78 1.57 3.79l.03.03c.26.26.4.6.4.97v2.93c0 .14-.11.25-.25.25H3.75a.25.25 0 0 1-.25-.25v-2.93c0-.37.14-.71.4-.97l.03-.03c1-1 1.57-2.37 1.57-3.79z" stroke="currentColor" stroke-linejoin="round"></path></svg></div></a></div><div class="l" aria-hidden="false"><button class="ax ei am ab q ao ej ek el" aria-label="user options menu" data-testid="headerUserIcon"><div class="l ee"><div class="l ee"><img alt="Sayam Kumar" class="l eq bx by bz cw" src="https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png" width="32" height="32" loading="lazy"/><div class="em bx l by bz eo n ax ep"></div></div></div></button></div></div></div><div class="l"><div class="er es et eu ev l"><div class="ab ca"><div class="ch bg ew ex ey ez"></div></div><article><div class="l"><div class="l"><span class="l"></span><section><div><div class="eo ff fg fh fi fj"></div><div class="fk fl fm fn fo"><div class="ab ca"><div class="ch bg ew ex ey ez"><div><h1 id="5ca4" class="pw-post-title fp fq fr be fs ft fu fv fw fx fy fz ga gb gc gd ge gf gg gh gi gj gk gl gm gn bj" data-testid="storyTitle">Transformers</h1></div><div><h2 id="7754" class="pw-subtitle-paragraph go fq fr be b gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cp dw">Or as I like to call it Attention on Steroids. 💉💊</h2><div class="he hf hg hh hi"><div class="speechify-ignore ab co"><div class="speechify-ignore bg l"><div class="hj hk hl hm hn ab"><div><div class="ab ho"><a href="https://ria-kulshrestha.medium.com/?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><div><div class="bl" aria-hidden="false"><div class="l hp hq bx hr hs"><div class="l ee"><img alt="Ria Kulshrestha" class="l eq bx dc dd cw" src="https://miro.medium.com/v2/resize:fill:88:88/1*fi1_Zj3QBbXTB3_B7kEv-w.png" width="44" height="44" loading="lazy" data-testid="authorPhoto"/><div class="ht bx l dc dd eo n hu ep"></div></div></div></div></div></a><a href="https://towardsdatascience.com/?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><div class="hv ab ee"><div><div class="bl" aria-hidden="false"><div class="l hw hx bx hr hy"><div class="l ee"><img alt="Towards Data Science" class="l eq bx bq hz cw" src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg" width="24" height="24" loading="lazy" data-testid="publicationPhoto"/><div class="ht bx l bq hz eo n hu ep"></div></div></div></div></div></div></a></div></div><div class="bm bg l"><div class="ab"><div style="flex:1"><span class="be b bf z bj"><div class="ia ab q"><div class="ab q ib"><div class="ab q"><div><div class="bl" aria-hidden="false"><p class="be b ic id bj"><a class="af ag ah ai aj ak al am an ao ap aq ar ie" data-testid="authorName" href="https://ria-kulshrestha.medium.com/?source=post_page-----89034557de14--------------------------------" rel="noopener follow">Ria Kulshrestha</a></p></div></div></div><span class="if ig" aria-hidden="true"><span class="be b bf z dw">·</span></span><p class="be b ic id dw"><button class="ih ii ah ai aj ak al am an ao ap aq ar ij ik il" disabled="">Follow</button></p></div></div></span></div></div><div class="l im"><span class="be b bf z dw"><div class="ab cm in io ip"><div class="iq ir ab"><div class="be b bf z dw ab is"><span class="it l im">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar ie ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><p class="be b bf z iu iv iw ix iy iz ja jb bj">Towards Data Science</p></a></div></div></div><div class="h k"><span class="if ig" aria-hidden="true"><span class="be b bf z dw">·</span></span></div></div><span class="be b bf z dw"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="jc jd l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="be b bf z dw">·</span></span></div><span data-testid="storyPublishDate">Jun 29, 2020</span></div></span></div></span></div></div></div><div class="ab co je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt"><div class="h k w eb ec q"><div class="kj l"><div class="ab q kk kl"><div class="pw-multi-vote-icon ee it km kn ko"><div class=""><div class="kp kq kr ks kt ku kv am kw kx ky ko"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l kz la lb lc ld le lf"><p class="be b dx z dw"><span class="kq">--</span></p></div></div></div><div><div class="bl" aria-hidden="false"><button class="ao kp li lj ab q ef lk ll" aria-label="responses"><svg width="24" height="24" viewBox="0 0 24 24" class="lh"><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b dx z dw"><span class="pw-responses-count lg lh">4</span></p></button></div></div></div><div class="ab q ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="lm k j i d"></div><div class="h k"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ef ah ai aj ak al ln an ao ap ij lo lp lq" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="lr"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div></div><div class="eq ls cm"><div class="l ae"><div class="ab ca"><div class="lt lu lv lw lx ly ch bg"><div class="ab"><div class="bl bg" aria-hidden="false"><div><div class="bl" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ef ah ai aj ak al ln an ao ap ij lz ma ll mb mc md me mf s mg mh mi mj mk ml mm u mn mo mp"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0zm9-10a10 10 0 1 0 0 20 10 10 0 0 0 0-20zm3.38 10.42l-4.6 3.06a.5.5 0 0 1-.78-.41V8.93c0-.4.45-.63.78-.41l4.6 3.06c.3.2.3.64 0 .84z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z dw">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ef ah ai aj ak al ln an ao ap ij lz ma ll mb mc md me mf s mg mh mi mj mk ml mm u mn mo mp"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z dw">Share</p></div></button></div></div></div><div class="bl" aria-hidden="false"><div><div class="bl" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ef ah ai aj ak al ln an ao ap ij lz ma ll mb mc md me mf s mg mh mi mj mk ml mm u mn mo mp"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg><div class="j i d"><p class="be b bf z dw">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mt mu mv mw mx my mq mr paragraph-image"><div role="button" tabindex="0" class="mz na ee nb bg nc"><div class="mq mr ms"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/0*2YfRrpQtt8TG2iuz 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*2YfRrpQtt8TG2iuz 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*2YfRrpQtt8TG2iuz 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*2YfRrpQtt8TG2iuz 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*2YfRrpQtt8TG2iuz 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*2YfRrpQtt8TG2iuz 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2YfRrpQtt8TG2iuz 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/0*2YfRrpQtt8TG2iuz 640w, https://miro.medium.com/v2/resize:fit:720/0*2YfRrpQtt8TG2iuz 720w, https://miro.medium.com/v2/resize:fit:750/0*2YfRrpQtt8TG2iuz 750w, https://miro.medium.com/v2/resize:fit:786/0*2YfRrpQtt8TG2iuz 786w, https://miro.medium.com/v2/resize:fit:828/0*2YfRrpQtt8TG2iuz 828w, https://miro.medium.com/v2/resize:fit:1100/0*2YfRrpQtt8TG2iuz 1100w, https://miro.medium.com/v2/resize:fit:1400/0*2YfRrpQtt8TG2iuz 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="A picture of Transformer from the movie franchise." class="bg ly nd c" width="700" height="394" loading="eager"/></picture></div></div><figcaption class="ne nf ng mq mr nh ni be b bf z dw">Photo by <a class="af nj" href="https://unsplash.com/@tetrakiss?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Arseny Togulev</a> on <a class="af nj" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2dd3" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">No, this article is not about the American sci-fi action movie series — no Optimus Prime here. It is also not about the electrical device that is used to transfer energy from one electrical circuit to another. What is this about then, you ask?</p><p id="63ee" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">It is about the one in the most sci-fi fields of all time, Artificial Intelligence — Natural Language Processing in particular and it is pretty optimal at transferring information and primely used. (See what I did there. :P)</p><p id="11ca" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">This post is based on the paper: <a class="af nj" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention is All You Need</a>. P.S. the authors were not kidding when they chose that title because you will need all the attention at your disposal for this one. But don’t let that scare you, it is SO SO worth it!!</p><h1 id="dd46" class="og oh fr be oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bj">What is a Transformer?</h1><p id="7bf2" class="pw-post-body-paragraph nk nl fr nm b gp pc no np gs pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fk bj">The Transformer in NLP is a <strong class="nm fs">novel architecture</strong> that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. It relies entirely on self-attention to compute representations of its input and output <strong class="nm fs">WITHOUT</strong> using sequence-aligned RNNs or convolution. 🤯</p><p id="051c" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">If you recall my previous post, <a class="af nj" rel="noopener" target="_blank" href="/attaining-attention-in-deep-learning-a712f93bdb1e">Understanding Attention In Deep Learning</a>, we discussed how and why many models fell short when it came to handling long-range dependencies. The concept of attention somewhat allowed us to overcome that problem and now in Transformers we will build on top of attention and unleash its full potential.</p><div class="ph pi pj pk pl pm"><a rel="noopener follow" target="_blank" href="/attaining-attention-in-deep-learning-a712f93bdb1e?source=post_page-----89034557de14--------------------------------"><div class="pn ab im"><div class="po ab cn ca pp pq"><h2 class="be fs ic z iu pr iw ix ps iz jb fq bj">Understanding Attention In Deep Learning</h2><div class="pt l"><h3 class="be b ic z iu pr iw ix ps iz jb dw">How a little attention changed the AI game!</h3></div><div class="pu l"><p class="be b dx z iu pr iw ix ps iz jb dw">towardsdatascience.com</p></div></div><div class="pv l"><div class="pw l px py pz pv qa ly pm"></div></div></div></a></div><h1 id="476c" class="og oh fr be oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bj">Few things to know before diving into Transformers</h1><h2 id="ae0b" class="qb oh fr be oi qc qd qe ol qf qg qh oo nt qi qj qk nx ql qm qn ob qo qp qq qr bj">Self-Attention</h2><p id="e7aa" class="pw-post-body-paragraph nk nl fr nm b gp pc no np gs pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fk bj">Let us start with revisiting what attention is in the NLP universe? <a class="af nj" rel="noopener" target="_blank" href="/attaining-attention-in-deep-learning-a712f93bdb1e">Understanding Attention In Deep Learning</a>. (<em class="qs">I apologize for these blatant self-advertisements, but seriously give it a read. It will help you under Transformers much better. I promise.</em>)</p><p id="4308" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj"><strong class="nm fs"><em class="qs">Attention allowed us to focus on parts of our input sequence while we predicted our output sequence</em></strong>. If our model predicted the word “<em class="qs">rouge</em>” [French translation for the color red], we are very likely to find a high weight-age for the word “<em class="qs">red</em>” in our input sequence. So attention, in a way, allowed us to map some connection/correlation between the input word “<em class="qs">rouge</em>” and the output word “<em class="qs">red</em>”.</p><blockquote class="qt qu qv"><p id="c722" class="nk nl qs nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj"><strong class="nm fs">Self attention</strong>, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.</p></blockquote><p id="3783" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">In simpler terms, <strong class="nm fs"><em class="qs">self attention helps us create similar connections but within the same sentence.</em></strong> Look at the following example:</p><pre class="mt mu mv mw mx qw qx qy qz ax ra bj"><span id="9333" class="qb oh fr qx b ic rb rc l is rd">“I poured water from the <em class="qs">bottle</em> into the <strong class="qx fs"><em class="qs">cup</em></strong> until <strong class="qx fs">it</strong> was <strong class="qx fs"><em class="qs">full</em></strong>.”<br/>it =&gt; cup</span><span id="5eb7" class="qb oh fr qx b ic re rc l is rd">“I poured water from the <strong class="qx fs">bottle</strong> into the cup until <strong class="qx fs">it</strong> was <strong class="qx fs"><em class="qs">empty</em></strong>.”<br/>it=&gt; bottle</span></pre><p id="1006" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">By changing one word “<em class="qs">full</em>” — &gt; “<em class="qs">empty</em>” the reference object for “<em class="qs">it</em>” changed. If we are translating such a sentence, we will want to know the word “<em class="qs">it</em>” refers to.</p><h2 id="3ccb" class="qb oh fr be oi qc qd qe ol qf qg qh oo nt qi qj qk nx ql qm qn ob qo qp qq qr bj">The three kinds of Attention possible in a model:</h2><ol class=""><li id="aa2b" class="nk nl fr nm b gp pc no np gs pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of rf rg rh bj"><strong class="nm fs"><em class="qs">Encoder-Decoder Attention</em>: </strong>Attention between the input sequence and the output sequence.</li><li id="1394" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rf rg rh bj"><strong class="nm fs"><em class="qs">Self attention in the input sequence</em>:</strong> Attends to all the words in the input sequence.</li><li id="4f0b" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rf rg rh bj"><strong class="nm fs"><em class="qs">Self attention in the output sequence:</em></strong> One thing we should be wary of here is that the scope of self attention is limited to the words that occur before a given word. This prevents any information leaks during the training of the model. This is done by masking the words that occur after it for each step. So for step 1, only the first word of the output sequence is NOT masked, for step 2, the first two words are NOT masked and so on.</li></ol><h2 id="c369" class="qb oh fr be oi qc qd qe ol qf qg qh oo nt qi qj qk nx ql qm qn ob qo qp qq qr bj">Keys, Values, and Queries:</h2><p id="1b38" class="pw-post-body-paragraph nk nl fr nm b gp pc no np gs pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fk bj">The three random words I just threw at you in this heading are vectors created as abstractions are useful for calculating self attention, more details on each below. These are calculated by multiplying your input vector(<em class="qs">X</em>) with weight matrices that are learnt while training.</p><ul class=""><li id="e989" class="nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of rn rg rh bj"><strong class="nm fs"><em class="qs">Query Vector</em></strong>: <em class="qs">q</em>= <em class="qs">X * Wq. </em>Think of this as the current word.</li><li id="d280" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rn rg rh bj"><strong class="nm fs"><em class="qs">Key Vector</em></strong><em class="qs">:</em> <em class="qs">k</em>= <em class="qs">X * Wk. </em>Think of this as an indexing mechanism for Value vector. Similar to how we have key-value pairs in hash maps, where keys are used to uniquely index the values.</li><li id="ad86" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rn rg rh bj"><strong class="nm fs"><em class="qs">Value Vector:</em></strong> <em class="qs">v</em>= <em class="qs">X * Wv. </em>Think of this as the information in the input word.</li></ul><p id="8897" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">What we want to do is take query <em class="qs">q </em>and find the most similar key <em class="qs">k</em>, by doing a dot product for <em class="qs">q </em>and <em class="qs">k</em>. The closest query-key product will have the highest value, followed by a softmax that will drive the <em class="qs">q.k </em>with smaller values close to 0 and <em class="qs">q.k </em>with larger values towards 1. This softmax distribution is multiplied with <em class="qs">v. </em>The value vectors multiplied with ~1 will get more attention while the ones ~0 will get less. The sizes of these <em class="qs">q, k </em>and<em class="qs"> v </em>vectors are referred to as “<strong class="nm fs"><em class="qs">hidden size</em></strong>” by various implementations.</p><figure class="mt mu mv mw mx my mq mr paragraph-image"><div role="button" tabindex="0" class="mz na ee nb bg nc"><div class="mq mr ro"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*rq5m4VKziz2CvblZ70sKjA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*rq5m4VKziz2CvblZ70sKjA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*rq5m4VKziz2CvblZ70sKjA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*rq5m4VKziz2CvblZ70sKjA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*rq5m4VKziz2CvblZ70sKjA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*rq5m4VKziz2CvblZ70sKjA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rq5m4VKziz2CvblZ70sKjA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*rq5m4VKziz2CvblZ70sKjA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*rq5m4VKziz2CvblZ70sKjA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*rq5m4VKziz2CvblZ70sKjA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*rq5m4VKziz2CvblZ70sKjA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*rq5m4VKziz2CvblZ70sKjA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*rq5m4VKziz2CvblZ70sKjA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*rq5m4VKziz2CvblZ70sKjA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="Showed how q and k are softmax-ed and then a dot product with gives us the final output." class="bg ly nd c" width="700" height="365" loading="lazy"/></picture></div></div><figcaption class="ne nf ng mq mr nh ni be b bf z dw">The values represent the index for q, k and i.</figcaption></figure><p id="3ff3" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">All these matrices <em class="qs">Wq, Wk</em> and <em class="qs">Wv</em> are learnt while being jointly trained during the model training.</p><h2 id="9e3e" class="qb oh fr be oi qc qd qe ol qf qg qh oo nt qi qj qk nx ql qm qn ob qo qp qq qr bj">Calculating Self attention from q, k and v:</h2><figure class="mt mu mv mw mx my mq mr paragraph-image"><div class="mq mr rp"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*-B9pI_0xI9M4air8x_ggYA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*-B9pI_0xI9M4air8x_ggYA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*-B9pI_0xI9M4air8x_ggYA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*-B9pI_0xI9M4air8x_ggYA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*-B9pI_0xI9M4air8x_ggYA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-B9pI_0xI9M4air8x_ggYA.png 1100w, https://miro.medium.com/v2/resize:fit:1274/format:webp/1*-B9pI_0xI9M4air8x_ggYA.png 1274w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 637px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*-B9pI_0xI9M4air8x_ggYA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*-B9pI_0xI9M4air8x_ggYA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*-B9pI_0xI9M4air8x_ggYA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*-B9pI_0xI9M4air8x_ggYA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*-B9pI_0xI9M4air8x_ggYA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*-B9pI_0xI9M4air8x_ggYA.png 1100w, https://miro.medium.com/v2/resize:fit:1274/1*-B9pI_0xI9M4air8x_ggYA.png 1274w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 637px"/><img alt="Formula for attention. Do Q dot K, divide by square root of dim of K. Take softmax and lastly multiply with V." class="bg ly nd c" width="637" height="110" loading="lazy"/></picture></div><figcaption class="ne nf ng mq mr nh ni be b bf z dw">Formula for self-attention. Source: paper.</figcaption></figure><p id="ba59" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">If we are calculating self attention for <em class="qs">#i</em> input word,</p><ul class=""><li id="7405" class="nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of rn rg rh bj"><strong class="nm fs"><em class="qs">Step 1:</em></strong> Multiply qᵢ by the kⱼ key vector of word.</li><li id="c614" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rn rg rh bj"><strong class="nm fs"><em class="qs">Step 2:</em></strong> Then divide this product by the square root of the dimension of key vector. <br/>This step is done<strong class="nm fs"> for better gradient flow </strong>which is specially important in cases when the value of the dot product in previous step is too big. As using them directly might push the softmax into regions with very little gradient flow.</li><li id="e669" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rn rg rh bj"><strong class="nm fs"><em class="qs">Step 3:</em></strong> Once we have scores for all <em class="qs">j</em>s, we pass these through a softmax. We get normalized value for each <em class="qs">j</em>.</li><li id="e009" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rn rg rh bj"><strong class="nm fs"><em class="qs">Step 4:</em></strong> Multiply softmax scores for each <em class="qs">j</em> with <em class="qs">v</em>ᵢ<em class="qs"> </em>vector. <br/>The idea/purpose here is, very similar attention, to keep preserve only the values <em class="qs">v </em>of the input word(s) we want to focus on by multiplying them with high probability scores from softmax ~1, and remove the rest by driving them towards 0, i.e. making them very small by multiplying them with the low probability scores ~0 from softmax.</li></ul><figure class="mt mu mv mw mx my mq mr paragraph-image"><div role="button" tabindex="0" class="mz na ee nb bg nc"><div class="mq mr rq"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*jbqL3JWQyLFVgNgYyOGKhg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*jbqL3JWQyLFVgNgYyOGKhg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*jbqL3JWQyLFVgNgYyOGKhg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*jbqL3JWQyLFVgNgYyOGKhg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*jbqL3JWQyLFVgNgYyOGKhg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*jbqL3JWQyLFVgNgYyOGKhg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jbqL3JWQyLFVgNgYyOGKhg.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*jbqL3JWQyLFVgNgYyOGKhg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*jbqL3JWQyLFVgNgYyOGKhg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*jbqL3JWQyLFVgNgYyOGKhg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*jbqL3JWQyLFVgNgYyOGKhg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*jbqL3JWQyLFVgNgYyOGKhg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*jbqL3JWQyLFVgNgYyOGKhg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*jbqL3JWQyLFVgNgYyOGKhg.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="The calculation at each step shown mathematically." class="bg ly nd c" width="700" height="608" loading="lazy"/></picture></div></div><figcaption class="ne nf ng mq mr nh ni be b bf z dw">Calculating output of self attention for the ith input word. If you are looking for an analogy between self attention and attention, think of z serving the purpose of context vectors and not global alignment weights.</figcaption></figure><h1 id="60b1" class="og oh fr be oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bj">The Transformer</h1><p id="14f9" class="pw-post-body-paragraph nk nl fr nm b gp pc no np gs pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fk bj">⚠️ <strong class="nm fs"><em class="qs">A word of caution</em></strong>: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2)</p></div></div><div class="my"><div class="ab ca"><div class="lt rr lu rs lv rt ce ru cf rv ch bg"><figure class="mt mu mv mw mx my rx ry paragraph-image"><div role="button" tabindex="0" class="mz na ee nb bg nc"><div class="mq mr rw"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*iy12bH-FiUNOy9-0bULgSg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*iy12bH-FiUNOy9-0bULgSg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*iy12bH-FiUNOy9-0bULgSg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*iy12bH-FiUNOy9-0bULgSg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*iy12bH-FiUNOy9-0bULgSg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*iy12bH-FiUNOy9-0bULgSg.png 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*iy12bH-FiUNOy9-0bULgSg.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*iy12bH-FiUNOy9-0bULgSg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*iy12bH-FiUNOy9-0bULgSg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*iy12bH-FiUNOy9-0bULgSg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*iy12bH-FiUNOy9-0bULgSg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*iy12bH-FiUNOy9-0bULgSg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*iy12bH-FiUNOy9-0bULgSg.png 1100w, https://miro.medium.com/v2/resize:fit:2000/1*iy12bH-FiUNOy9-0bULgSg.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"/><img alt="The architecture of Transformer from the paper with encoder, decoder and pre-processing parts labelled." class="bg ly nd c" width="1000" height="593" loading="eager"/></picture></div></div><figcaption class="ne nf ng mq mr nh ni be b bf z dw">(left) The Transformer architecture. Source: paper. (right) An abstracted version of the same for better understanding.</figcaption></figure></div></div></div><div class="ab ca"><div class="ch bg ew ex ey ez"><h2 id="a003" class="qb oh fr be oi qc qd qe ol qf qg qh oo nt qi qj qk nx ql qm qn ob qo qp qq qr bj">Beast #1: Encoder-Decoder stacks</h2><p id="f376" class="pw-post-body-paragraph nk nl fr nm b gp pc no np gs pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fk bj"><strong class="nm fs"><em class="qs">Encoder</em></strong>: The encoder maps an input sequence of symbol representations <em class="qs">(x</em>₁<em class="qs">, …, x</em>ₙ<em class="qs">)</em> to a sequence of representations <em class="qs">z = (z</em>₁<em class="qs">, …, z</em>ₙ<em class="qs">)</em>. Think of them as the outputs from self attention with some post-processing.</p><p id="697d" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">Each encoder has two sub-layers.</p><ol class=""><li id="0434" class="nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of rf rg rh bj">A <strong class="nm fs">multi-head self attention mechanism</strong> on the input vectors (Think parallelized and efficient sibling of self attention).</li><li id="06ba" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rf rg rh bj">A simple, position-wise <strong class="nm fs">fully connected feed-forward network</strong> (Think post-processing).</li></ol><p id="bc00" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">Check out <a class="af nj" href="https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/bert-encoder" rel="noopener ugc nofollow" target="_blank">this</a> absolute bomb 3D diagram of the Encoder block used in <a class="af nj" href="https://medium.com/@ria.kulshrestha16/keeping-up-with-the-berts-5b7beb92766" rel="noopener">BERT</a>. <strong class="nm fs">Seriously you can’t miss this!!! </strong>It is like a whole new level of understanding.</p><p id="beb8" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj"><strong class="nm fs"><em class="qs">Decoder</em></strong>: Given <em class="qs">z</em>, the decoder then generates an output sequence <em class="qs">(y</em>₁<em class="qs">, …, y</em>ₘ<em class="qs">)</em> of symbols one element at a time.</p><p id="4a5b" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">Each decoder has three sub-layers.</p><ol class=""><li id="f4a0" class="nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of rf rg rh bj">A <strong class="nm fs"><em class="qs">masked</em></strong> <strong class="nm fs">multi-head self attention mechanism </strong>on the output vectors of the previous iteration.</li><li id="8ce0" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rf rg rh bj">A <strong class="nm fs">multi-head attention mechanism </strong>on the output from encoder and masked multi-headed attention in decoder.</li><li id="3025" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rf rg rh bj">A simple, position-wise <strong class="nm fs">fully connected feed-forward network</strong> (think post-processing).</li></ol><p id="cdba" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">A few additional points:</p><ul class=""><li id="7f0d" class="nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of rn rg rh bj">In the original paper, 6 layers were present in the encoder stack (2 sub-layer version) and 6 in the decoder stack (3 sub-layer version).</li><li id="c3dc" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rn rg rh bj">All sub-layers in the model, as well as the embedding layers, produce outputs of the same dimension. This is done to facilitate the residual connections.</li></ul><h2 id="bde0" class="qb oh fr be oi qc qd qe ol qf qg qh oo nt qi qj qk nx ql qm qn ob qo qp qq qr bj">Beast #2 Inside Encoder-Decoder stacks — Multi-Head Attention:</h2><figure class="mt mu mv mw mx my mq mr paragraph-image"><div role="button" tabindex="0" class="mz na ee nb bg nc"><div class="mq mr rz"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*9UJYSFUpPlx1cLMPCm7ACw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*9UJYSFUpPlx1cLMPCm7ACw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*9UJYSFUpPlx1cLMPCm7ACw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*9UJYSFUpPlx1cLMPCm7ACw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*9UJYSFUpPlx1cLMPCm7ACw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9UJYSFUpPlx1cLMPCm7ACw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9UJYSFUpPlx1cLMPCm7ACw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*9UJYSFUpPlx1cLMPCm7ACw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*9UJYSFUpPlx1cLMPCm7ACw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*9UJYSFUpPlx1cLMPCm7ACw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*9UJYSFUpPlx1cLMPCm7ACw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*9UJYSFUpPlx1cLMPCm7ACw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*9UJYSFUpPlx1cLMPCm7ACw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*9UJYSFUpPlx1cLMPCm7ACw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="Diagram showing where each kind of attention is calculated in the architecture." class="bg ly nd c" width="700" height="574" loading="lazy"/></picture></div></div><figcaption class="ne nf ng mq mr nh ni be b bf z dw">The three kinds of attention in encoder and decoder stacks along with feed forward neural networks.</figcaption></figure><p id="5a00" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">We just noted that the output of each sub-layer needs to be of the same dimension which is 512 in our paper.<br/>=&gt; zᵢ needs to be of 512 dimensions. <br/>=&gt; vᵢ needs to be of 512 dimensions as zᵢ are just sort of weighted sums of vᵢs.</p><p id="3c06" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">Additionally, we want to allow the model to focus on different positions is by <strong class="nm fs">calculating self attention multiple times with different sets</strong> of <em class="qs">q, k </em>and <em class="qs">v</em> vectors, then take an average of all those outputs to get our final <em class="qs">z</em>.</p><p id="6920" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">So instead of dealing with these humongous vectors and averaging multiple outputs, we reduce the size of our <em class="qs">k,q</em> and <em class="qs">v</em> vectors to some smaller dimension — reduces size of <em class="qs">Wq, Wk</em>, and <em class="qs">Wv</em> matrices as well. We keep the multiple sets (<em class="qs">h</em>) of <em class="qs">k,q</em> and <em class="qs">v and </em>refer to each set as an “<em class="qs">attention head”,</em> hence the name <em class="qs">multi-headed</em> attention. And lastly, instead of averaging to get final <em class="qs">z</em>, we concatenate them.</p><p id="e329" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">The size of the concatenated vector will be too large to be fed to the next sub-layer, so we scale it down by multiplying it with another learnt matrix <em class="qs">Wo</em>.</p></div></div><div class="my"><div class="ab ca"><div class="lt rr lu rs lv rt ce ru cf rv ch bg"><figure class="mt mu mv mw mx my rx ry paragraph-image"><div role="button" tabindex="0" class="mz na ee nb bg nc"><div class="mq mr sa"><picture><source srcSet="https://miro.medium.com/v2/resize:fit:640/format:webp/1*NMTM422Crac6Joostx0rOw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*NMTM422Crac6Joostx0rOw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*NMTM422Crac6Joostx0rOw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*NMTM422Crac6Joostx0rOw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*NMTM422Crac6Joostx0rOw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*NMTM422Crac6Joostx0rOw.png 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*NMTM422Crac6Joostx0rOw.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"/><source data-testid="og" srcSet="https://miro.medium.com/v2/resize:fit:640/1*NMTM422Crac6Joostx0rOw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*NMTM422Crac6Joostx0rOw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*NMTM422Crac6Joostx0rOw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*NMTM422Crac6Joostx0rOw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*NMTM422Crac6Joostx0rOw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*NMTM422Crac6Joostx0rOw.png 1100w, https://miro.medium.com/v2/resize:fit:2000/1*NMTM422Crac6Joostx0rOw.png 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"/><img alt="Steps to calculate dot product attention and multi headed attention, taken from the paper." class="bg ly nd c" width="1000" height="489" loading="eager"/></picture></div></div><figcaption class="ne nf ng mq mr nh ni be b bf z dw">(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. Source: paper.</figcaption></figure></div></div></div><div class="ab ca"><div class="ch bg ew ex ey ez"><p id="0e56" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">Multiple attention heads allowed the model to jointly attend to information from different representation sub-spaces at different positions which was inhibited by averaging in a single attention head.</p><h2 id="5b84" class="qb oh fr be oi qc qd qe ol qf qg qh oo nt qi qj qk nx ql qm qn ob qo qp qq qr bj">Beast #3— Input and Output Pre-processing:</h2><p id="209d" class="pw-post-body-paragraph nk nl fr nm b gp pc no np gs pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fk bj">The input words are represented using some form of embedding. This is done for both encoder and decoder.</p><p id="d051" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">Word embedding on their own lack any positional information which is achieved in RNNs by virtue of their sequential nature. Meanwhile in self-attention, due to softmax, any such positional information is lost.</p><p id="7e8b" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">To preserve the positional information, the transformer injects a vector to individual input embeddings (could be using word embeddings for corresponding to the input words). These vectors follow a specific periodic function (Example: combination of various sines/cosines having different frequency, in short not in sync with each other) that the model learns and is able to <strong class="nm fs">determine the position of individual word wrt each other </strong>based on the values <strong class="nm fs">.</strong></p><p id="5da5" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">This injected vector is called “<em class="qs">positional encoding</em>” and are added to the input embeddings at the bottoms of both encoder and decoder stacks.</p><h2 id="e88f" class="qb oh fr be oi qc qd qe ol qf qg qh oo nt qi qj qk nx ql qm qn ob qo qp qq qr bj">Beast #4 — Decoder stack: Revisited</h2><p id="f01c" class="pw-post-body-paragraph nk nl fr nm b gp pc no np gs pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fk bj">The output of the decoder stack at each step is fed back to the decoder in the next time step — pretty similar to how outputs from previous steps in RNNs were used as next hidden states. And just as we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to preserve the position of each word. This positional encoding + word embedding combo is then fed into a masked multi-headed self attention.</p><p id="3a58" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">This self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions — you can’t look at future words. This masking ensures that the predictions for position <em class="qs">i</em> can depend only on the known outputs at positions less than <em class="qs">i</em>.</p><p id="eb24" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">The outputs from the encoder stack are then used as multiple sets of key vectors <strong class="nm fs"><em class="qs">k </em></strong>and value vectors <strong class="nm fs"><em class="qs">v</em></strong>, for the “encoder decoder attention” — shown in green in the diagram — layer. It helps the decoder focus on the contextually relevant parts in the input sequence for that step. (The part similar to global attention vectors.) The <strong class="nm fs"><em class="qs">q</em></strong><em class="qs"> </em>vector comes from the “output self attention” layer.</p><p id="795d" class="pw-post-body-paragraph nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">Once we get the output from the decoder, we do a softmax again to select the final probabilities of words.</p><h1 id="170e" class="og oh fr be oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bj">Conclusion</h1><p id="dd13" class="pw-post-body-paragraph nk nl fr nm b gp pc no np gs pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fk bj">Let’s finish with a quick wrap-up revision.</p><ul class=""><li id="061f" class="nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of rn rg rh bj">We started with understanding what self attention is and how to calculate self-attention from these <em class="qs">v, k ,q </em>vectors.</li><li id="4680" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rn rg rh bj">Multi-headed attention is an efficient modification of self attention that uses multiple smaller sets of <em class="qs">v, k ,q </em>and concatenates the outputs from each set to get the final <em class="qs">z.</em></li><li id="c8a2" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rn rg rh bj">Then we saw how and where the three kinds of self attention are used in the model.</li><li id="e629" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rn rg rh bj">Followed by the pre-processing done on the inputs for the encoder and decoder stacks.</li></ul></div></div></div><div class="ab ca sb sc sd se" role="separator"><span class="sf bx bl sg sh si"></span><span class="sf bx bl sg sh si"></span><span class="sf bx bl sg sh"></span></div><div class="fk fl fm fn fo"><div class="ab ca"><div class="ch bg ew ex ey ez"><h1 id="585c" class="og oh fr be oi oj sj gr ol om sk gu oo op sl or os ot sm ov ow ox sn oz pa pb bj">References + Recommended Reads</h1><ul class=""><li id="c6d3" class="nk nl fr nm b gp pc no np gs pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of rn rg rh bj"><a class="af nj" rel="noopener" target="_blank" href="/attaining-attention-in-deep-learning-a712f93bdb1e">Understanding Deep Attention in Deep Learning</a> if you faced issues around attention.</li><li id="006c" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rn rg rh bj"><a class="af nj" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">The Illustrated Transformer</a> — Has great visualizations along with the explanations.</li><li id="d54e" class="nk nl fr nm b gp ri no np gs rj nr ns nt rk nv nw nx rl nz oa ob rm od oe of rn rg rh bj"><a class="af nj" href="https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/" rel="noopener ugc nofollow" target="_blank">https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/</a></li></ul><figure class="mt mu mv mw mx my"><div class="so iu l ee"><div class="sp sq l"></div></div></figure><ul class=""><li id="6a53" class="nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of rn rg rh bj">YouTube video #1: A great resource to get the model intuition. Specially if you want to <strong class="nm fs">learn more about positional encoding.</strong></li></ul><figure class="mt mu mv mw mx my"><div class="so iu l ee"><div class="sp sq l"></div></div></figure><ul class=""><li id="4f7b" class="nk nl fr nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of rn rg rh bj">YouTube video#2: Talk by Lukasz Kaiser on this <strong class="nm fs">paper explaining self attention</strong> — . P.S. He is one of the authors for this paper.</li></ul></div></div></div><div class="ab ca sb sc sd se" role="separator"><span class="sf bx bl sg sh si"></span><span class="sf bx bl sg sh si"></span><span class="sf bx bl sg sh"></span></div><div class="fk fl fm fn fo"><div class="ab ca"><div class="ch bg ew ex ey ez"><blockquote class="qt qu qv"><p id="c98f" class="nk nl qs nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">I’m glad you made it till the end of this article. <em class="fr">🎉</em><br/>I hope your reading experience was as enriching as the one I had writing this. <em class="fr">💖</em></p><p id="2f6c" class="nk nl qs nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">If you liked this article, do check out my other ones <a class="af nj" href="https://medium.com/@ria.kulshrestha16" rel="noopener">here</a>.</p><p id="ec84" class="nk nl qs nm b gp nn no np gs nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fk bj">If you want to reach out to me, my medium of choice would be <a class="af nj" href="https://twitter.com/ree_____ree" rel="noopener ugc nofollow" target="_blank">Twitter</a>.</p></blockquote></div></div></div></div></section></div></div></article><div class="ab ca"><div class="ch bg ew ex ey ez"></div></div></div><div class="ab ca"><div class="ch bg ew ex ey ez"><div class="sr ss ab ip"><div class="pt ab"><a class="st ax am ao" href="https://medium.com/tag/artificial-intelligence?source=post_page-----89034557de14---------------artificial_intelligence-----------------" rel="noopener follow"><div class="su ee cw sv fb sw sx be b bf z bj sy">Artificial Intelligence</div></a></div><div class="pt ab"><a class="st ax am ao" href="https://medium.com/tag/machine-learning?source=post_page-----89034557de14---------------machine_learning-----------------" rel="noopener follow"><div class="su ee cw sv fb sw sx be b bf z bj sy">Machine Learning</div></a></div><div class="pt ab"><a class="st ax am ao" href="https://medium.com/tag/nlp?source=post_page-----89034557de14---------------nlp-----------------" rel="noopener follow"><div class="su ee cw sv fb sw sx be b bf z bj sy">NLP</div></a></div><div class="pt ab"><a class="st ax am ao" href="https://medium.com/tag/transformers?source=post_page-----89034557de14---------------transformers-----------------" rel="noopener follow"><div class="su ee cw sv fb sw sx be b bf z bj sy">Transformers</div></a></div><div class="pt ab"><a class="st ax am ao" href="https://medium.com/tag/ai?source=post_page-----89034557de14---------------ai-----------------" rel="noopener follow"><div class="su ee cw sv fb sw sx be b bf z bj sy">AI</div></a></div></div></div></div><div class="l"></div><footer class="sz ta tb tc td te tf tg th ab q ti hy c"><div class="l ae"><div class="ab ca"><div class="ch bg ew ex ey ez"><div class="ab co tj"><div class="ab q kk"><div class="tk l"><span class="l tl tm tn e d"><div class="ab q kk kl"><div class="pw-multi-vote-icon ee it km kn ko"><div class=""><div class="kp kq kr ks kt ku kv am kw kx ky ko"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l kz la lb lc ld le lf"><p class="be b dx z dw"><span class="kq">--</span></p></div></div></span><span class="l h g f to tp"><div class="ab q kk kl"><div class="pw-multi-vote-icon ee it km kn ko"><div class=""><div class="kp kq kr ks kt ku kv am kw kx ky ko"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM15.42 1.84l-1.18-.39-.34 2.5 1.52-2.1zM9.76 1.45l-1.19.4 1.53 2.1-.34-2.5zM20.25 11.84l-2.5-4.4a1.42 1.42 0 0 0-.93-.64.96.96 0 0 0-.75.18c-.25.19-.4.42-.45.7l.05.05 2.35 4.13c1.62 2.95 1.1 5.78-1.52 8.4l-.46.41c1-.13 1.93-.6 2.78-1.45 2.7-2.7 2.51-5.59 1.43-7.38zM12.07 9.01c-.13-.69.08-1.3.57-1.77l-2.06-2.07a1.12 1.12 0 0 0-1.56 0c-.15.15-.22.34-.27.53L12.07 9z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M14.74 8.3a1.13 1.13 0 0 0-.73-.5.67.67 0 0 0-.53.13c-.15.12-.59.46-.2 1.3l1.18 2.5a.45.45 0 0 1-.23.76.44.44 0 0 1-.48-.25L7.6 6.11a.82.82 0 1 0-1.15 1.15l3.64 3.64a.45.45 0 1 1-.63.63L5.83 7.9 4.8 6.86a.82.82 0 0 0-1.33.9c.04.1.1.18.18.26l1.02 1.03 3.65 3.64a.44.44 0 0 1-.15.73.44.44 0 0 1-.48-.1L4.05 9.68a.82.82 0 0 0-1.4.57.81.81 0 0 0 .24.58l1.53 1.54 2.3 2.28a.45.45 0 0 1-.64.63L3.8 13a.81.81 0 0 0-1.39.57c0 .22.09.43.24.58l4.4 4.4c2.8 2.8 5.5 4.12 8.68.94 2.27-2.28 2.71-4.6 1.34-7.1l-2.32-4.08z"></path></svg></div></div></div><div class="pw-multi-vote-count l kz la lb lc ld le lf"><p class="be b dx z dw"><span class="kq">--</span></p></div></div></span></div><div class="bp ab"><div><div class="bl" aria-hidden="false"><button class="ao kp li lj ab q ef lk ll" aria-label="responses"><svg width="24" height="24" viewBox="0 0 24 24" class="lh"><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="be b bf z dw"><span class="pw-responses-count lg lh">4</span></p></button></div></div></div></div><div class="ab q"><div class="si l im"><div><div class="bl" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="footerBookmarkButton" class="af ef ah ai aj ak al ln an ao ap ij lo lp lq" disabled=""><svg width="24" height="24" viewBox="0 0 24 24" fill="none" class="lr"><path d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5v-2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V5.75z" fill="#000"></path></svg></button></div></div></div><div class="si l im"><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="footerSocialShareButton" class="af ef ah ai aj ak al ln an ao ap ij lz ma ll mb"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg></button></div></div></div></div><div class="bl" aria-hidden="false"><div><div class="bl" aria-hidden="false"><button aria-label="More options" data-testid="footerStoryOptionsButton" class="af ef ah ai aj ak al ln an ao ap ij lz ma ll mb"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.39 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.4.59.56 0 1.03-.2 1.42-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.6-1.41A1.93 1.93 0 0 0 6.4 10c-.55 0-1.02.2-1.41.59-.4.39-.6.86-.6 1.41zM10 12c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.54 0 1.02-.2 1.4-.59.4-.39.6-.86.6-1.41 0-.55-.2-1.02-.6-1.41a1.93 1.93 0 0 0-1.4-.59c-.55 0-1.04.2-1.42.59-.4.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.57 1.41.4.4.88.59 1.43.59.57 0 1.04-.2 1.43-.59.39-.39.57-.86.57-1.41 0-.55-.2-1.02-.57-1.41A1.93 1.93 0 0 0 17.6 10c-.55 0-1.04.2-1.43.59-.38.39-.57.86-.57 1.41z" fill="currentColor"></path></svg></button></div></div></div></div></div></div></div></div></footer><div class="tq tr ts tt tu l bw"><div class="ab ca"><div class="ch bg ew ex ey ez"><div class="ck ab tv co"><div class="ab ho"><a href="https://ria-kulshrestha.medium.com/?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><div class="l tw tx bx ty hs"><div class="l ee"><img alt="Ria Kulshrestha" class="l eq bx tz ua cw" src="https://miro.medium.com/v2/resize:fill:144:144/1*fi1_Zj3QBbXTB3_B7kEv-w.png" width="72" height="72" loading="lazy"/><div class="ht bx l tz ua eo n hu ep"></div></div></div></a><a href="https://towardsdatascience.com/?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><div class="ub ab ee"><div><div class="bl" aria-hidden="false"><div class="l uc ud bx ty hy"><div class="l ee"><img alt="Towards Data Science" class="l eq bx by bz cw" src="https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg" width="32" height="32" loading="lazy"/><div class="ht bx l by bz eo n hu ep"></div></div></div></div></div></div></a></div><div class="j i d"><div class="ab"><button class="be b bf z ue su uf ra ug uh ui uj uk ul um ij un uo up uq ur us eq bl ut nf">Follow</button><div class="dv l"><div><div><div class="bl" aria-hidden="false"><div class="l"><button class="be b bf z ue am uf ra ug uh ui uj uk ul um ij un uo up ur us eq bl ut nf" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="uu ud uc"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg></button></div></div></div></div></div></div></div></div><div class="ab cm co"><div class="l"><div class="ab q"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab q" href="https://ria-kulshrestha.medium.com/?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><h2 class="pw-author-name be uv uw ux uy bj"><span class="fk">Written by <!-- -->Ria Kulshrestha</span></h2></a></div><div class="pt ab"><div class="l im"><span class="pw-follower-count be b bf z bj"><a class="af ag ah ai aj ak al am an ao ap aq ar ie" href="https://ria-kulshrestha.medium.com/followers?source=post_page-----89034557de14--------------------------------" rel="noopener follow">933 Followers</a></span></div><div class="be b bf z iu iv iw ab iy iz ja jb dw is"><span class="if l" aria-hidden="true"><span class="be b bf z dw">·</span></span><span class="l im">Writer for </span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar ie ab q" href="https://towardsdatascience.com/?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><p class="be b bf z iu iv iw ix iy iz ja jb bj">Towards Data Science</p></a></div></div></div></div><div class="uz l"><p class="be b bf z bj"><span class="fk">AI enthusiast currently exploring SE @Google. Claps/Shares/Comments are appreciated💖https://twitter.com/Ree_____Ree</span></p></div></div><div class="h k"><div class="ab"><button class="be b bf z ue su uf ra ug uh ui uj uk ul um ij un uo up uq ur us eq bl ut nf">Follow</button><div class="dv l"><div><div><div class="bl" aria-hidden="false"><div class="l"><button class="be b bf z ue am uf ra ug uh ui uj uk ul um ij un uo up ur us eq bl ut nf" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="uu ud uc"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg></button></div></div></div></div></div></div></div></div><div class="va bg vb vc vd ve vf vg"></div></div></div><div class="h k j"><div class="va bg vb vh"></div><div class="ab ca"><div class="ch bg ew ex ey ez"><div class="vi ab kk ip"><div class="vj vk l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://help.medium.com/hc/en-us?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><p class="be b dx z dw">Help</p></a></div><div class="vj vk l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.statuspage.io/?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><p class="be b dx z dw">Status</p></a></div><div class="vj vk l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/about?autoplay=1&amp;source=post_page-----89034557de14--------------------------------" rel="noopener follow"><p class="be b dx z dw">About</p></a></div><div class="vj vk l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><p class="be b dx z dw">Careers</p></a></div><div class="vj vk l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://blog.medium.com/?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><p class="be b dx z dw">Blog</p></a></div><div class="vj vk l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><p class="be b dx z dw">Privacy</p></a></div><div class="vj vk l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><p class="be b dx z dw">Terms</p></a></div><div class="vj vk l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://speechify.com/medium?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><p class="be b dx z dw">Text to speech</p></a></div><div class="vj l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/business?source=post_page-----89034557de14--------------------------------" rel="noopener follow"><p class="be b dx z dw">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20240423-141906-9fd62d4f46"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"cache":{"experimentGroupSet":true,"reason":"User is logged in","group":"disabled","tags":["group-edgeCachePosts","post-89034557de14","user-406aa3cbd38e","collection-7f60cf5620c9"],"serverVariantState":"","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":false,"vary":[],"loHomepageEnabled":false,"updatedPostPreviewsEnabled":false,"customMocPreviewWeightThreshold":"control","recommendedTagsQueryEnabled":false,"enableLohpWithSearch":"control","enableLohpFocused":"control","enableMobileLohpShortHero":"control","enableSpamBuster":"control"},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"isFirefox":false,"routingEntity":{"type":"COLLECTION","id":"7f60cf5620c9","explicit":true},"viewerIsBot":false},"debug":{"requestId":"cc550d76-b0ba-42a5-8ee5-d2cc5b48c4e8","hybridDevServices":[],"originalSpanCarrier":{"ot-tracer-spanid":"45b27bb774dcb09d","ot-tracer-traceid":"2c6fcc522757bfc5","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Ftransformers-89034557de14","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false},"config":{"nodeEnv":"production","version":"main-20240423-141906-9fd62d4f46","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","recaptchaEnterpriseKeyId":"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20240423-141906-9fd62d4f46","commit":"9fd62d4f46755ea14962e3f08667f5cdffc5abde"}},"datacenter":"us"},"googleAnalyticsCode":"G-7JY7T788PK","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumMastodonDomainName":"me.dm","mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyV2":"e8a5e126-792b-4ee6-8fba-d574c1b02fc5","monthlyWithTrial":"d5ee3dbe3db8","monthlyPremium":"fa741a9b47a2","yearly":"a40ad4a43185","yearlyV2":"3815d7d6-b8ca-4224-9b8c-182f9047866e","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","yearlyPremium":"e21bd2c12166","monthlyOneYearFree":"e6c0637a-2bad-4171-ab4f-3c268633d83c","monthly25PercentOffFirstYear":"235ecc62-0cdb-49ae-9378-726cd21c504b","monthly20PercentOffFirstYear":"ba518864-9c13-4a99-91ca-411bf0cac756","monthly15PercentOffFirstYear":"594c029b-9f89-43d5-88f8-8173af4e070e","monthly10PercentOffFirstYear":"c6c7bc9a-40f2-4b51-8126-e28511d5bdb0","monthlyForStudents":"629ebe51-da7d-41fd-8293-34cd2f2030a8","yearlyOneYearFree":"78ba7be9-0d9f-4ece-aa3e-b54b826f2bf1","yearly25PercentOffFirstYear":"2dbb010d-bb8f-4eeb-ad5c-a08509f42d34","yearly20PercentOffFirstYear":"47565488-435b-47f8-bf93-40d5fbe0ebc8","yearly15PercentOffFirstYear":"8259809b-0881-47d9-acf7-6c001c7f720f","yearly10PercentOffFirstYear":"9dd694fb-96e1-472c-8d9e-3c868d5c1506","yearlyForStudents":"e29345ef-ab1c-4234-95c5-70e50fe6bc23","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06","fiftyPercentOffOneYear":"FIFTY_PERCENT_OFF_ONE_YEAR"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","q8qw":"usd","d9y6":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"},"imageUploadMaxSizeMb":25,"staffPicks":{"title":"Staff Picks","catalogId":"c7bc6e1ee00f"}},"session":{"xsrf":"7cff875a44f5"}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","isLoggedIn":true,"variantFlags":[{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"covid_19_cdc_banner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pill_based_home_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_reading_history","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_editor_new_publishing_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_diversification_rex","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_syntax_highlight","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_trial_membership","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_avatar_upload","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_seamless_social_sharing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_import","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mobile_lohp_short_hero","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefined_top_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_deprecate_legacy_providers_v3","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lohp_with_search","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"enable_android_dynamic_aspirational_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_group_gifting","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pre_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sharer_validate_post_share_key","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_user_onboarding_emails_flow","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_verifications_service","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_play_purchase_on_backend","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"enable_susi_redesign_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_archive_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_premium_plan","valueType":{"__typename":"VariantFlagString","value":"4a442ace1476"}},{"__typename":"VariantFlag","name":"crm_send_contact_to_sendgrid","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"can_receive_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_dashboard_referred_earnings","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"rex_generator_max_candidates","valueType":{"__typename":"VariantFlagNumber"}},{"__typename":"VariantFlag","name":"android_two_hour_refresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_updated_new_user_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_friend_links_creation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_spam_buster","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"limit_user_follows","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_maim_the_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sharer_create_post_share_key","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_susi_redesign_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"enable_author_cards","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_expired_membership_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reengagement_notification_duration","valueType":{"__typename":"VariantFlagNumber"}},{"__typename":"VariantFlag","name":"android_enable_topic_portals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_entities_to_follow_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_recirc_model","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"enable_android_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_premium_tier_badge","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_eventstats_event_processing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_verified_book_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"price_smoke_test_monthly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_newsletter_lo_flow_custom_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_continue_this_thread","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_new_push_notification_endpoint","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound-source-serif-pro"}},{"__typename":"VariantFlag","name":"enable_android_dynamic_programming_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_display_paywall_after_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_miro_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_auto_follow_on_subscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_verified_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automod","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"price_smoke_test_yearly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"can_send_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_monthly_premium_plan","valueType":{"__typename":"VariantFlagString","value":"12a660186432"}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_premium_tier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_social_share_sheet","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"onboarding_tags_from_top_views","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_aspiriational","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_friend_links_creation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"enable_custom_moc_preview_weight_threshold","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_lists_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_autorefresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members_username_selection","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_simplified_digest_v2_b","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_widget","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_google_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lohp_focused","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards_byline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_recaptcha_enterprise","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_remove_twitter_onboarding_step","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"textshots_userid","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_cache_less_following_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_easy_resubscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_switch_plan_premium_tier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_iceland_forced_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"web_enable_syntax_highlighting","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reader_fair_distribution_non_qp","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tag_recs","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_friend_links_postpage_banners","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_response_markup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_image_sharer","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_tagline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_legacy_feed_in_iceland","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_members_only_audio","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_creator_welcome_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mps_pp_writer_stats","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_rating_prompt_stories_read_threshold","valueType":{"__typename":"VariantFlagNumber"}},{"__typename":"VariantFlag","name":"enable_moc_load_processor_c","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"skip_fs_cache_user_vals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_programming","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sprig","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_aggregator_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_partner_program_enrollment","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_in_app_free_trial","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_custom_moc_preview_weight_threshold_li","valueType":{"__typename":"VariantFlagString","value":"group_1"}},{"__typename":"VariantFlag","name":"enable_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_iceland_nux","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_apple_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_friend_links_postpage_banners","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"viewer":{"__ref":"User:fca54767663"},"collectionByDomainOrSlug({\"domainOrSlug\":\"towardsdatascience.com\"})":{"__ref":"Collection:7f60cf5620c9"},"postResult({\"id\":\"89034557de14\"})":{"__ref":"Post:89034557de14"}},"UserViewerEdge:userId:fca54767663-viewerId:fca54767663":{"__typename":"UserViewerEdge","id":"userId:fca54767663-viewerId:fca54767663","createdAt":1616936306259},"User:fca54767663":{"__typename":"User","id":"fca54767663","allowEmailAddressSharingEditorWriter":false,"atsQualifiedAt":0,"dismissableFlags":["WRITER_SUBSCRIPTIONS_TOOLTIP","CREATE_CATALOGS_CTA"],"emailObfuscated":"sa•••••••@gmail.com","geolocation":{"__typename":"Geolocation","country":"CA"},"hasGroupGiftingEnabled":false,"hasPastMemberships":false,"hasSubdomain":false,"imageId":"","isEligibleToImportEmails":false,"isEligibleToViewNewResponses":true,"isMembershipTrialEligible":true,"isSuspended":false,"membership":null,"name":"Sayam Kumar","partnerProgramEnrollment":null,"postSubscribeMembershipUpsellShownAt":0,"styleEditorOnboardingVersionSeen":0,"twitterScreenName":"","unverifiedEmail":"","username":"sayamk565","viewerEdge":{"__ref":"UserViewerEdge:userId:fca54767663-viewerId:fca54767663"}},"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png":{"__typename":"ImageMetadata","id":"1*VzTUkfeGymHP4Bvav-T-lA.png"},"Collection:7f60cf5620c9":{"__typename":"Collection","id":"7f60cf5620c9","favicon":{"__ref":"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png"},"customStyleSheet":null,"colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"googleAnalyticsId":null,"editors":[{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:7e12c71dfa81"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:e6ad8abedec9"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:895063a310f4"}}],"name":"Towards Data Science","avatar":{"__ref":"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg"},"domain":"towardsdatascience.com","slug":"towards-data-science","description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","subscriberCount":688562,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:fca54767663"},"twitterUsername":"TDataScience","facebookPageId":null,"logo":{"__ref":"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png"}},"User:7e12c71dfa81":{"__typename":"User","id":"7e12c71dfa81"},"User:e6ad8abedec9":{"__typename":"User","id":"e6ad8abedec9"},"User:895063a310f4":{"__typename":"User","id":"895063a310f4"},"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg":{"__typename":"ImageMetadata","id":"1*CJe3891yB1A1mzMdqemkdg.jpeg"},"LinkedAccounts:406aa3cbd38e":{"__typename":"LinkedAccounts","mastodon":null,"id":"406aa3cbd38e"},"UserViewerEdge:userId:406aa3cbd38e-viewerId:fca54767663":{"__typename":"UserViewerEdge","id":"userId:406aa3cbd38e-viewerId:fca54767663","isFollowing":false,"isUser":false,"isMuting":false},"NewsletterV3:ccdcb0ec19a4":{"__typename":"NewsletterV3","id":"ccdcb0ec19a4","type":"NEWSLETTER_TYPE_AUTHOR","slug":"406aa3cbd38e","name":"406aa3cbd38e","collection":null,"user":{"__ref":"User:406aa3cbd38e"}},"User:406aa3cbd38e":{"__typename":"User","id":"406aa3cbd38e","name":"Ria Kulshrestha","username":"ria-kulshrestha","newsletterV3":{"__ref":"NewsletterV3:ccdcb0ec19a4"},"linkedAccounts":{"__ref":"LinkedAccounts:406aa3cbd38e"},"isSuspended":false,"imageId":"1*fi1_Zj3QBbXTB3_B7kEv-w.png","mediumMemberAt":0,"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"socialStats":{"__typename":"SocialStats","followerCount":933},"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"ria-kulshrestha.medium.com"}},"hasSubdomain":true,"bio":"AI enthusiast currently exploring SE @Google. Claps\u002FShares\u002FComments are appreciated💖https:\u002F\u002Ftwitter.com\u002FRee_____Ree","isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:406aa3cbd38e-viewerId:fca54767663"},"viewerIsUser":false,"postSubscribeMembershipUpsellShownAt":0,"allowNotes":true,"membership":null,"twitterScreenName":""},"Topic:1eca0103fff3":{"__typename":"Topic","slug":"machine-learning","id":"1eca0103fff3","name":"Machine Learning"},"Paragraph:25349c1228ec_0":{"__typename":"Paragraph","id":"25349c1228ec_0","name":"5ca4","type":"H3","href":null,"layout":null,"metadata":null,"text":"Transformers","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_1":{"__typename":"Paragraph","id":"25349c1228ec_1","name":"7754","type":"H4","href":null,"layout":null,"metadata":null,"text":"Or as I like to call it Attention on Steroids. 💉💊","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:0*2YfRrpQtt8TG2iuz":{"__typename":"ImageMetadata","id":"0*2YfRrpQtt8TG2iuz","originalHeight":4472,"originalWidth":7952,"focusPercentX":null,"focusPercentY":null,"alt":"A picture of Transformer from the movie franchise."},"Paragraph:25349c1228ec_2":{"__typename":"Paragraph","id":"25349c1228ec_2","name":"5f40","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:0*2YfRrpQtt8TG2iuz"},"text":"Photo by Arseny Togulev on Unsplash","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":9,"end":23,"href":"https:\u002F\u002Funsplash.com\u002F@tetrakiss?utm_source=medium&utm_medium=referral","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":27,"end":35,"href":"https:\u002F\u002Funsplash.com?utm_source=medium&utm_medium=referral","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_3":{"__typename":"Paragraph","id":"25349c1228ec_3","name":"2dd3","type":"P","href":null,"layout":null,"metadata":null,"text":"No, this article is not about the American sci-fi action movie series — no Optimus Prime here. It is also not about the electrical device that is used to transfer energy from one electrical circuit to another. What is this about then, you ask?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_4":{"__typename":"Paragraph","id":"25349c1228ec_4","name":"63ee","type":"P","href":null,"layout":null,"metadata":null,"text":"It is about the one in the most sci-fi fields of all time, Artificial Intelligence — Natural Language Processing in particular and it is pretty optimal at transferring information and primely used. (See what I did there. :P)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_5":{"__typename":"Paragraph","id":"25349c1228ec_5","name":"11ca","type":"P","href":null,"layout":null,"metadata":null,"text":"This post is based on the paper: Attention is All You Need. P.S. the authors were not kidding when they chose that title because you will need all the attention at your disposal for this one. But don’t let that scare you, it is SO SO worth it!!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":33,"end":58,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1706.03762","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_6":{"__typename":"Paragraph","id":"25349c1228ec_6","name":"dd46","type":"H3","href":null,"layout":null,"metadata":null,"text":"What is a Transformer?","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_7":{"__typename":"Paragraph","id":"25349c1228ec_7","name":"7bf2","type":"P","href":null,"layout":null,"metadata":null,"text":"The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. It relies entirely on self-attention to compute representations of its input and output WITHOUT using sequence-aligned RNNs or convolution. 🤯","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":28,"end":46,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":231,"end":238,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_8":{"__typename":"Paragraph","id":"25349c1228ec_8","name":"051c","type":"P","href":null,"layout":null,"metadata":null,"text":"If you recall my previous post, Understanding Attention In Deep Learning, we discussed how and why many models fell short when it came to handling long-range dependencies. The concept of attention somewhat allowed us to overcome that problem and now in Transformers we will build on top of attention and unleash its full potential.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":32,"end":72,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Fattaining-attention-in-deep-learning-a712f93bdb1e","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_9":{"__typename":"Paragraph","id":"25349c1228ec_9","name":"6219","type":"MIXTAPE_EMBED","href":null,"layout":null,"metadata":null,"text":"Understanding Attention In Deep Learning\nHow a little attention changed the AI game!towardsdatascience.com","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":106,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Fattaining-attention-in-deep-learning-a712f93bdb1e","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":41,"end":84,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":{"__typename":"MixtapeMetadata","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fattaining-attention-in-deep-learning-a712f93bdb1e","mediaResource":{"__typename":"MediaResource","mediumCatalog":null},"thumbnailImageId":"0*3WEpWAkyXjUHbNbG"}},"Paragraph:25349c1228ec_10":{"__typename":"Paragraph","id":"25349c1228ec_10","name":"476c","type":"H3","href":null,"layout":null,"metadata":null,"text":"Few things to know before diving into Transformers","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_11":{"__typename":"Paragraph","id":"25349c1228ec_11","name":"ae0b","type":"H4","href":null,"layout":null,"metadata":null,"text":"Self-Attention","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_12":{"__typename":"Paragraph","id":"25349c1228ec_12","name":"e7aa","type":"P","href":null,"layout":null,"metadata":null,"text":"Let us start with revisiting what attention is in the NLP universe? Understanding Attention In Deep Learning. (I apologize for these blatant self-advertisements, but seriously give it a read. It will help you under Transformers much better. I promise.)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":68,"end":108,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Fattaining-attention-in-deep-learning-a712f93bdb1e","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":111,"end":251,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_13":{"__typename":"Paragraph","id":"25349c1228ec_13","name":"4308","type":"P","href":null,"layout":null,"metadata":null,"text":"Attention allowed us to focus on parts of our input sequence while we predicted our output sequence. If our model predicted the word “rouge” [French translation for the color red], we are very likely to find a high weight-age for the word “red” in our input sequence. So attention, in a way, allowed us to map some connection\u002Fcorrelation between the input word “rouge” and the output word “red”.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":99,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":99,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":134,"end":139,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":240,"end":243,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":362,"end":367,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":390,"end":393,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_14":{"__typename":"Paragraph","id":"25349c1228ec_14","name":"c722","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Self attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_15":{"__typename":"Paragraph","id":"25349c1228ec_15","name":"3783","type":"P","href":null,"layout":null,"metadata":null,"text":"In simpler terms, self attention helps us create similar connections but within the same sentence. Look at the following example:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":18,"end":98,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":18,"end":98,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_16":{"__typename":"Paragraph","id":"25349c1228ec_16","name":"9333","type":"PRE","href":null,"layout":null,"metadata":null,"text":"“I poured water from the bottle into the cup until it was full.”\nit =\u003E cup","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":41,"end":44,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":51,"end":53,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":58,"end":62,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":25,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":41,"end":44,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":58,"end":62,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_17":{"__typename":"Paragraph","id":"25349c1228ec_17","name":"5eb7","type":"PRE","href":null,"layout":null,"metadata":null,"text":"“I poured water from the bottle into the cup until it was empty.”\nit=\u003E bottle","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":25,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":51,"end":53,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":58,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":58,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_18":{"__typename":"Paragraph","id":"25349c1228ec_18","name":"1006","type":"P","href":null,"layout":null,"metadata":null,"text":"By changing one word “full” — \u003E “empty” the reference object for “it” changed. If we are translating such a sentence, we will want to know the word “it” refers to.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":22,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":33,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":66,"end":68,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":149,"end":151,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_19":{"__typename":"Paragraph","id":"25349c1228ec_19","name":"3ccb","type":"H4","href":null,"layout":null,"metadata":null,"text":"The three kinds of Attention possible in a model:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_20":{"__typename":"Paragraph","id":"25349c1228ec_20","name":"aa2b","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Encoder-Decoder Attention: Attention between the input sequence and the output sequence.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_21":{"__typename":"Paragraph","id":"25349c1228ec_21","name":"1394","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Self attention in the input sequence: Attends to all the words in the input sequence.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_22":{"__typename":"Paragraph","id":"25349c1228ec_22","name":"4f0b","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Self attention in the output sequence: One thing we should be wary of here is that the scope of self attention is limited to the words that occur before a given word. This prevents any information leaks during the training of the model. This is done by masking the words that occur after it for each step. So for step 1, only the first word of the output sequence is NOT masked, for step 2, the first two words are NOT masked and so on.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_23":{"__typename":"Paragraph","id":"25349c1228ec_23","name":"c369","type":"H4","href":null,"layout":null,"metadata":null,"text":"Keys, Values, and Queries:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_24":{"__typename":"Paragraph","id":"25349c1228ec_24","name":"1b38","type":"P","href":null,"layout":null,"metadata":null,"text":"The three random words I just threw at you in this heading are vectors created as abstractions are useful for calculating self attention, more details on each below. These are calculated by multiplying your input vector(X) with weight matrices that are learnt while training.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":220,"end":221,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_25":{"__typename":"Paragraph","id":"25349c1228ec_25","name":"e989","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Query Vector: q= X * Wq. Think of this as the current word.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":14,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":17,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_26":{"__typename":"Paragraph","id":"25349c1228ec_26","name":"d280","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Key Vector: k= X * Wk. Think of this as an indexing mechanism for Value vector. Similar to how we have key-value pairs in hash maps, where keys are used to uniquely index the values.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":12,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":15,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_27":{"__typename":"Paragraph","id":"25349c1228ec_27","name":"ad86","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Value Vector: v= X * Wv. Think of this as the information in the input word.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":14,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":17,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_28":{"__typename":"Paragraph","id":"25349c1228ec_28","name":"8897","type":"P","href":null,"layout":null,"metadata":null,"text":"What we want to do is take query q and find the most similar key k, by doing a dot product for q and k. The closest query-key product will have the highest value, followed by a softmax that will drive the q.k with smaller values close to 0 and q.k with larger values towards 1. This softmax distribution is multiplied with v. The value vectors multiplied with ~1 will get more attention while the ones ~0 will get less. The sizes of these q, k and v vectors are referred to as “hidden size” by various implementations.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":478,"end":489,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":33,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":65,"end":66,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":95,"end":97,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":101,"end":102,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":205,"end":209,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":244,"end":248,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":323,"end":326,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":439,"end":444,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":447,"end":450,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":478,"end":489,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*rq5m4VKziz2CvblZ70sKjA.png":{"__typename":"ImageMetadata","id":"1*rq5m4VKziz2CvblZ70sKjA.png","originalHeight":608,"originalWidth":1168,"focusPercentX":null,"focusPercentY":null,"alt":"Showed how q and k are softmax-ed and then a dot product with gives us the final output."},"Paragraph:25349c1228ec_29":{"__typename":"Paragraph","id":"25349c1228ec_29","name":"e789","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*rq5m4VKziz2CvblZ70sKjA.png"},"text":"The values represent the index for q, k and i.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_30":{"__typename":"Paragraph","id":"25349c1228ec_30","name":"3ff3","type":"P","href":null,"layout":null,"metadata":null,"text":"All these matrices Wq, Wk and Wv are learnt while being jointly trained during the model training.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":19,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":30,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_31":{"__typename":"Paragraph","id":"25349c1228ec_31","name":"9e3e","type":"H4","href":null,"layout":null,"metadata":null,"text":"Calculating Self attention from q, k and v:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*-B9pI_0xI9M4air8x_ggYA.png":{"__typename":"ImageMetadata","id":"1*-B9pI_0xI9M4air8x_ggYA.png","originalHeight":110,"originalWidth":637,"focusPercentX":null,"focusPercentY":null,"alt":"Formula for attention. Do Q dot K, divide by square root of dim of K. Take softmax and lastly multiply with V."},"Paragraph:25349c1228ec_32":{"__typename":"Paragraph","id":"25349c1228ec_32","name":"e691","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*-B9pI_0xI9M4air8x_ggYA.png"},"text":"Formula for self-attention. Source: paper.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_33":{"__typename":"Paragraph","id":"25349c1228ec_33","name":"ba59","type":"P","href":null,"layout":null,"metadata":null,"text":"If we are calculating self attention for #i input word,","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":41,"end":43,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_34":{"__typename":"Paragraph","id":"25349c1228ec_34","name":"7405","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Step 1: Multiply qᵢ by the kⱼ key vector of word.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_35":{"__typename":"Paragraph","id":"25349c1228ec_35","name":"c614","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Step 2: Then divide this product by the square root of the dimension of key vector. \nThis step is done for better gradient flow which is specially important in cases when the value of the dot product in previous step is too big. As using them directly might push the softmax into regions with very little gradient flow.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":102,"end":128,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_36":{"__typename":"Paragraph","id":"25349c1228ec_36","name":"e669","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Step 3: Once we have scores for all js, we pass these through a softmax. We get normalized value for each j.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":36,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":106,"end":107,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_37":{"__typename":"Paragraph","id":"25349c1228ec_37","name":"e009","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Step 4: Multiply softmax scores for each j with vᵢ vector. \nThe idea\u002Fpurpose here is, very similar attention, to keep preserve only the values v of the input word(s) we want to focus on by multiplying them with high probability scores from softmax ~1, and remove the rest by driving them towards 0, i.e. making them very small by multiplying them with the low probability scores ~0 from softmax.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":41,"end":42,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":48,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":50,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":143,"end":145,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*jbqL3JWQyLFVgNgYyOGKhg.png":{"__typename":"ImageMetadata","id":"1*jbqL3JWQyLFVgNgYyOGKhg.png","originalHeight":645,"originalWidth":743,"focusPercentX":null,"focusPercentY":null,"alt":"The calculation at each step shown mathematically."},"Paragraph:25349c1228ec_38":{"__typename":"Paragraph","id":"25349c1228ec_38","name":"f120","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*jbqL3JWQyLFVgNgYyOGKhg.png"},"text":"Calculating output of self attention for the ith input word. If you are looking for an analogy between self attention and attention, think of z serving the purpose of context vectors and not global alignment weights.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_39":{"__typename":"Paragraph","id":"25349c1228ec_39","name":"60b1","type":"H3","href":null,"layout":null,"metadata":null,"text":"The Transformer","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_40":{"__typename":"Paragraph","id":"25349c1228ec_40","name":"14f9","type":"P","href":null,"layout":null,"metadata":null,"text":"⚠️ A word of caution: the contents of this image may appear exponentially more complicated than they are. We will break this scary beast down into small baby beasts and it will all make sense. (I promise #2)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":3,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":3,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*iy12bH-FiUNOy9-0bULgSg.png":{"__typename":"ImageMetadata","id":"1*iy12bH-FiUNOy9-0bULgSg.png","originalHeight":972,"originalWidth":1640,"focusPercentX":null,"focusPercentY":null,"alt":"The architecture of Transformer from the paper with encoder, decoder and pre-processing parts labelled."},"Paragraph:25349c1228ec_41":{"__typename":"Paragraph","id":"25349c1228ec_41","name":"d550","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*iy12bH-FiUNOy9-0bULgSg.png"},"text":"(left) The Transformer architecture. Source: paper. (right) An abstracted version of the same for better understanding.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_42":{"__typename":"Paragraph","id":"25349c1228ec_42","name":"a003","type":"H4","href":null,"layout":null,"metadata":null,"text":"Beast #1: Encoder-Decoder stacks","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_43":{"__typename":"Paragraph","id":"25349c1228ec_43","name":"f376","type":"P","href":null,"layout":null,"metadata":null,"text":"Encoder: The encoder maps an input sequence of symbol representations (x₁, …, xₙ) to a sequence of representations z = (z₁, …, zₙ). Think of them as the outputs from self attention with some post-processing.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":70,"end":72,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":73,"end":79,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":80,"end":81,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":115,"end":121,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":122,"end":128,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":129,"end":130,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_44":{"__typename":"Paragraph","id":"25349c1228ec_44","name":"697d","type":"P","href":null,"layout":null,"metadata":null,"text":"Each encoder has two sub-layers.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_45":{"__typename":"Paragraph","id":"25349c1228ec_45","name":"0434","type":"OLI","href":null,"layout":null,"metadata":null,"text":"A multi-head self attention mechanism on the input vectors (Think parallelized and efficient sibling of self attention).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":2,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_46":{"__typename":"Paragraph","id":"25349c1228ec_46","name":"06ba","type":"OLI","href":null,"layout":null,"metadata":null,"text":"A simple, position-wise fully connected feed-forward network (Think post-processing).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":24,"end":60,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_47":{"__typename":"Paragraph","id":"25349c1228ec_47","name":"bc00","type":"P","href":null,"layout":null,"metadata":null,"text":"Check out this absolute bomb 3D diagram of the Encoder block used in BERT. Seriously you can’t miss this!!! It is like a whole new level of understanding.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":10,"end":14,"href":"https:\u002F\u002Fpeltarion.com\u002Fknowledge-center\u002Fdocumentation\u002Fmodeling-view\u002Fbuild-an-ai-model\u002Fblocks\u002Fbert-encoder","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":69,"end":73,"href":"https:\u002F\u002Fmedium.com\u002F@ria.kulshrestha16\u002Fkeeping-up-with-the-berts-5b7beb92766","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":75,"end":108,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_48":{"__typename":"Paragraph","id":"25349c1228ec_48","name":"beb8","type":"P","href":null,"layout":null,"metadata":null,"text":"Decoder: Given z, the decoder then generates an output sequence (y₁, …, yₘ) of symbols one element at a time.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":15,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":64,"end":66,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":67,"end":73,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":74,"end":75,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_49":{"__typename":"Paragraph","id":"25349c1228ec_49","name":"4a5b","type":"P","href":null,"layout":null,"metadata":null,"text":"Each decoder has three sub-layers.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_50":{"__typename":"Paragraph","id":"25349c1228ec_50","name":"f4a0","type":"OLI","href":null,"layout":null,"metadata":null,"text":"A masked multi-head self attention mechanism on the output vectors of the previous iteration.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":2,"end":8,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":9,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":2,"end":8,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_51":{"__typename":"Paragraph","id":"25349c1228ec_51","name":"8ce0","type":"OLI","href":null,"layout":null,"metadata":null,"text":"A multi-head attention mechanism on the output from encoder and masked multi-headed attention in decoder.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":2,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_52":{"__typename":"Paragraph","id":"25349c1228ec_52","name":"3025","type":"OLI","href":null,"layout":null,"metadata":null,"text":"A simple, position-wise fully connected feed-forward network (think post-processing).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":24,"end":60,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_53":{"__typename":"Paragraph","id":"25349c1228ec_53","name":"cdba","type":"P","href":null,"layout":null,"metadata":null,"text":"A few additional points:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_54":{"__typename":"Paragraph","id":"25349c1228ec_54","name":"7f0d","type":"ULI","href":null,"layout":null,"metadata":null,"text":"In the original paper, 6 layers were present in the encoder stack (2 sub-layer version) and 6 in the decoder stack (3 sub-layer version).","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_55":{"__typename":"Paragraph","id":"25349c1228ec_55","name":"c3dc","type":"ULI","href":null,"layout":null,"metadata":null,"text":"All sub-layers in the model, as well as the embedding layers, produce outputs of the same dimension. This is done to facilitate the residual connections.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_56":{"__typename":"Paragraph","id":"25349c1228ec_56","name":"bde0","type":"H4","href":null,"layout":null,"metadata":null,"text":"Beast #2 Inside Encoder-Decoder stacks — Multi-Head Attention:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*9UJYSFUpPlx1cLMPCm7ACw.png":{"__typename":"ImageMetadata","id":"1*9UJYSFUpPlx1cLMPCm7ACw.png","originalHeight":1025,"originalWidth":1251,"focusPercentX":null,"focusPercentY":null,"alt":"Diagram showing where each kind of attention is calculated in the architecture."},"Paragraph:25349c1228ec_57":{"__typename":"Paragraph","id":"25349c1228ec_57","name":"f718","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*9UJYSFUpPlx1cLMPCm7ACw.png"},"text":"The three kinds of attention in encoder and decoder stacks along with feed forward neural networks.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_58":{"__typename":"Paragraph","id":"25349c1228ec_58","name":"5a00","type":"P","href":null,"layout":null,"metadata":null,"text":"We just noted that the output of each sub-layer needs to be of the same dimension which is 512 in our paper.\n=\u003E zᵢ needs to be of 512 dimensions. \n=\u003E vᵢ needs to be of 512 dimensions as zᵢ are just sort of weighted sums of vᵢs.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_59":{"__typename":"Paragraph","id":"25349c1228ec_59","name":"3c06","type":"P","href":null,"layout":null,"metadata":null,"text":"Additionally, we want to allow the model to focus on different positions is by calculating self attention multiple times with different sets of q, k and v vectors, then take an average of all those outputs to get our final z.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":79,"end":140,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":144,"end":149,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":153,"end":154,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":223,"end":224,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_60":{"__typename":"Paragraph","id":"25349c1228ec_60","name":"6920","type":"P","href":null,"layout":null,"metadata":null,"text":"So instead of dealing with these humongous vectors and averaging multiple outputs, we reduce the size of our k,q and v vectors to some smaller dimension — reduces size of Wq, Wk, and Wv matrices as well. We keep the multiple sets (h) of k,q and v and refer to each set as an “attention head”, hence the name multi-headed attention. And lastly, instead of averaging to get final z, we concatenate them.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":109,"end":112,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":117,"end":118,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":171,"end":177,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":183,"end":185,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":231,"end":232,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":237,"end":240,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":245,"end":251,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":276,"end":292,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":308,"end":320,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":378,"end":379,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_61":{"__typename":"Paragraph","id":"25349c1228ec_61","name":"e329","type":"P","href":null,"layout":null,"metadata":null,"text":"The size of the concatenated vector will be too large to be fed to the next sub-layer, so we scale it down by multiplying it with another learnt matrix Wo.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":152,"end":154,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*NMTM422Crac6Joostx0rOw.png":{"__typename":"ImageMetadata","id":"1*NMTM422Crac6Joostx0rOw.png","originalHeight":592,"originalWidth":1212,"focusPercentX":null,"focusPercentY":null,"alt":"Steps to calculate dot product attention and multi headed attention, taken from the paper."},"Paragraph:25349c1228ec_62":{"__typename":"Paragraph","id":"25349c1228ec_62","name":"d132","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*NMTM422Crac6Joostx0rOw.png"},"text":"(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. Source: paper.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_63":{"__typename":"Paragraph","id":"25349c1228ec_63","name":"0e56","type":"P","href":null,"layout":null,"metadata":null,"text":"Multiple attention heads allowed the model to jointly attend to information from different representation sub-spaces at different positions which was inhibited by averaging in a single attention head.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_64":{"__typename":"Paragraph","id":"25349c1228ec_64","name":"5b84","type":"H4","href":null,"layout":null,"metadata":null,"text":"Beast #3— Input and Output Pre-processing:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_65":{"__typename":"Paragraph","id":"25349c1228ec_65","name":"209d","type":"P","href":null,"layout":null,"metadata":null,"text":"The input words are represented using some form of embedding. This is done for both encoder and decoder.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_66":{"__typename":"Paragraph","id":"25349c1228ec_66","name":"d051","type":"P","href":null,"layout":null,"metadata":null,"text":"Word embedding on their own lack any positional information which is achieved in RNNs by virtue of their sequential nature. Meanwhile in self-attention, due to softmax, any such positional information is lost.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_67":{"__typename":"Paragraph","id":"25349c1228ec_67","name":"7e8b","type":"P","href":null,"layout":null,"metadata":null,"text":"To preserve the positional information, the transformer injects a vector to individual input embeddings (could be using word embeddings for corresponding to the input words). These vectors follow a specific periodic function (Example: combination of various sines\u002Fcosines having different frequency, in short not in sync with each other) that the model learns and is able to determine the position of individual word wrt each other based on the values .","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":375,"end":432,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":452,"end":453,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_68":{"__typename":"Paragraph","id":"25349c1228ec_68","name":"5da5","type":"P","href":null,"layout":null,"metadata":null,"text":"This injected vector is called “positional encoding” and are added to the input embeddings at the bottoms of both encoder and decoder stacks.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":32,"end":51,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_69":{"__typename":"Paragraph","id":"25349c1228ec_69","name":"e88f","type":"H4","href":null,"layout":null,"metadata":null,"text":"Beast #4 — Decoder stack: Revisited","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_70":{"__typename":"Paragraph","id":"25349c1228ec_70","name":"f01c","type":"P","href":null,"layout":null,"metadata":null,"text":"The output of the decoder stack at each step is fed back to the decoder in the next time step — pretty similar to how outputs from previous steps in RNNs were used as next hidden states. And just as we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to preserve the position of each word. This positional encoding + word embedding combo is then fed into a masked multi-headed self attention.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_71":{"__typename":"Paragraph","id":"25349c1228ec_71","name":"3a58","type":"P","href":null,"layout":null,"metadata":null,"text":"This self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions — you can’t look at future words. This masking ensures that the predictions for position i can depend only on the known outputs at positions less than i.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":212,"end":213,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":274,"end":275,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_72":{"__typename":"Paragraph","id":"25349c1228ec_72","name":"eb24","type":"P","href":null,"layout":null,"metadata":null,"text":"The outputs from the encoder stack are then used as multiple sets of key vectors k and value vectors v, for the “encoder decoder attention” — shown in green in the diagram — layer. It helps the decoder focus on the contextually relevant parts in the input sequence for that step. (The part similar to global attention vectors.) The q vector comes from the “output self attention” layer.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":81,"end":83,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":101,"end":102,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":332,"end":333,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":81,"end":83,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":101,"end":102,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":332,"end":334,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_73":{"__typename":"Paragraph","id":"25349c1228ec_73","name":"795d","type":"P","href":null,"layout":null,"metadata":null,"text":"Once we get the output from the decoder, we do a softmax again to select the final probabilities of words.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_74":{"__typename":"Paragraph","id":"25349c1228ec_74","name":"170e","type":"H3","href":null,"layout":null,"metadata":null,"text":"Conclusion","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_75":{"__typename":"Paragraph","id":"25349c1228ec_75","name":"dd13","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s finish with a quick wrap-up revision.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_76":{"__typename":"Paragraph","id":"25349c1228ec_76","name":"061f","type":"ULI","href":null,"layout":null,"metadata":null,"text":"We started with understanding what self attention is and how to calculate self-attention from these v, k ,q vectors.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":100,"end":108,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_77":{"__typename":"Paragraph","id":"25349c1228ec_77","name":"4680","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Multi-headed attention is an efficient modification of self attention that uses multiple smaller sets of v, k ,q and concatenates the outputs from each set to get the final z.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":105,"end":113,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":173,"end":175,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_78":{"__typename":"Paragraph","id":"25349c1228ec_78","name":"c8a2","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Then we saw how and where the three kinds of self attention are used in the model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_79":{"__typename":"Paragraph","id":"25349c1228ec_79","name":"e629","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Followed by the pre-processing done on the inputs for the encoder and decoder stacks.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_80":{"__typename":"Paragraph","id":"25349c1228ec_80","name":"585c","type":"H3","href":null,"layout":null,"metadata":null,"text":"References + Recommended Reads","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_81":{"__typename":"Paragraph","id":"25349c1228ec_81","name":"c6d3","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Understanding Deep Attention in Deep Learning if you faced issues around attention.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":45,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Fattaining-attention-in-deep-learning-a712f93bdb1e","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_82":{"__typename":"Paragraph","id":"25349c1228ec_82","name":"006c","type":"ULI","href":null,"layout":null,"metadata":null,"text":"The Illustrated Transformer — Has great visualizations along with the explanations.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":27,"href":"http:\u002F\u002Fjalammar.github.io\u002Fillustrated-transformer\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_83":{"__typename":"Paragraph","id":"25349c1228ec_83","name":"d54e","type":"ULI","href":null,"layout":null,"metadata":null,"text":"https:\u002F\u002Fwww.analyticsvidhya.com\u002Fblog\u002F2019\u002F06\u002Funderstanding-transformers-nlp-state-of-the-art-models\u002F","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":100,"href":"https:\u002F\u002Fwww.analyticsvidhya.com\u002Fblog\u002F2019\u002F06\u002Funderstanding-transformers-nlp-state-of-the-art-models\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:8219452f400a63ace3780aae70b1bbc0":{"__typename":"MediaResource","id":"8219452f400a63ace3780aae70b1bbc0","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FiDulhoQ2pro%3Ffeature%3Doembed&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DiDulhoQ2pro&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FiDulhoQ2pro%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=youtube","iframeHeight":480,"iframeWidth":854,"title":"Attention Is All You Need"},"Paragraph:25349c1228ec_84":{"__typename":"Paragraph","id":"25349c1228ec_84","name":"bfee","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:8219452f400a63ace3780aae70b1bbc0"}},"mixtapeMetadata":null},"Paragraph:25349c1228ec_85":{"__typename":"Paragraph","id":"25349c1228ec_85","name":"6a53","type":"ULI","href":null,"layout":null,"metadata":null,"text":"YouTube video #1: A great resource to get the model intuition. Specially if you want to learn more about positional encoding.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":88,"end":125,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"MediaResource:a51379ef1805b85aa6460bbf57d77d71":{"__typename":"MediaResource","id":"a51379ef1805b85aa6460bbf57d77d71","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FrBCqOTEfxvg%3Ffeature%3Doembed&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DrBCqOTEfxvg&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FrBCqOTEfxvg%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=youtube","iframeHeight":480,"iframeWidth":854,"title":"Attention is all you need; Attentional Neural Network Models | Łukasz Kaiser | Masterclass"},"Paragraph:25349c1228ec_86":{"__typename":"Paragraph","id":"25349c1228ec_86","name":"b5c1","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:a51379ef1805b85aa6460bbf57d77d71"}},"mixtapeMetadata":null},"Paragraph:25349c1228ec_87":{"__typename":"Paragraph","id":"25349c1228ec_87","name":"4f7b","type":"ULI","href":null,"layout":null,"metadata":null,"text":"YouTube video#2: Talk by Lukasz Kaiser on this paper explaining self attention — . P.S. He is one of the authors for this paper.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":47,"end":78,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_88":{"__typename":"Paragraph","id":"25349c1228ec_88","name":"c98f","type":"BQ","href":null,"layout":null,"metadata":null,"text":"I’m glad you made it till the end of this article. 🎉\nI hope your reading experience was as enriching as the one I had writing this. 💖","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":51,"end":53,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":133,"end":135,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_89":{"__typename":"Paragraph","id":"25349c1228ec_89","name":"2f6c","type":"BQ","href":null,"layout":null,"metadata":null,"text":"If you liked this article, do check out my other ones here.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":54,"end":58,"href":"https:\u002F\u002Fmedium.com\u002F@ria.kulshrestha16","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:25349c1228ec_90":{"__typename":"Paragraph","id":"25349c1228ec_90","name":"ec84","type":"BQ","href":null,"layout":null,"metadata":null,"text":"If you want to reach out to me, my medium of choice would be Twitter.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":61,"end":68,"href":"https:\u002F\u002Ftwitter.com\u002Free_____ree","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:fca54767663":{"__typename":"CollectionViewerEdge","id":"collectionId:7f60cf5620c9-viewerId:fca54767663","isEditor":false,"isMuting":false},"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png":{"__typename":"ImageMetadata","id":"1*cFFKn8rFH4ZndmaYeAs6iQ.png","originalWidth":2381,"originalHeight":743},"Tag:artificial-intelligence":{"__typename":"Tag","id":"artificial-intelligence","displayTitle":"Artificial Intelligence","normalizedTagSlug":"artificial-intelligence"},"Tag:machine-learning":{"__typename":"Tag","id":"machine-learning","displayTitle":"Machine Learning","normalizedTagSlug":"machine-learning"},"Tag:nlp":{"__typename":"Tag","id":"nlp","displayTitle":"NLP","normalizedTagSlug":"nlp"},"Tag:transformers":{"__typename":"Tag","id":"transformers","displayTitle":"Transformers","normalizedTagSlug":"transformers"},"Tag:ai":{"__typename":"Tag","id":"ai","displayTitle":"AI","normalizedTagSlug":"ai"},"Post:89034557de14":{"__typename":"Post","id":"89034557de14","collection":{"__ref":"Collection:7f60cf5620c9"},"content({\"postMeteringOptions\":{}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"e2ef","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"1079","startIndex":80,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"a411","startIndex":88,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:25349c1228ec_0"},{"__ref":"Paragraph:25349c1228ec_1"},{"__ref":"Paragraph:25349c1228ec_2"},{"__ref":"Paragraph:25349c1228ec_3"},{"__ref":"Paragraph:25349c1228ec_4"},{"__ref":"Paragraph:25349c1228ec_5"},{"__ref":"Paragraph:25349c1228ec_6"},{"__ref":"Paragraph:25349c1228ec_7"},{"__ref":"Paragraph:25349c1228ec_8"},{"__ref":"Paragraph:25349c1228ec_9"},{"__ref":"Paragraph:25349c1228ec_10"},{"__ref":"Paragraph:25349c1228ec_11"},{"__ref":"Paragraph:25349c1228ec_12"},{"__ref":"Paragraph:25349c1228ec_13"},{"__ref":"Paragraph:25349c1228ec_14"},{"__ref":"Paragraph:25349c1228ec_15"},{"__ref":"Paragraph:25349c1228ec_16"},{"__ref":"Paragraph:25349c1228ec_17"},{"__ref":"Paragraph:25349c1228ec_18"},{"__ref":"Paragraph:25349c1228ec_19"},{"__ref":"Paragraph:25349c1228ec_20"},{"__ref":"Paragraph:25349c1228ec_21"},{"__ref":"Paragraph:25349c1228ec_22"},{"__ref":"Paragraph:25349c1228ec_23"},{"__ref":"Paragraph:25349c1228ec_24"},{"__ref":"Paragraph:25349c1228ec_25"},{"__ref":"Paragraph:25349c1228ec_26"},{"__ref":"Paragraph:25349c1228ec_27"},{"__ref":"Paragraph:25349c1228ec_28"},{"__ref":"Paragraph:25349c1228ec_29"},{"__ref":"Paragraph:25349c1228ec_30"},{"__ref":"Paragraph:25349c1228ec_31"},{"__ref":"Paragraph:25349c1228ec_32"},{"__ref":"Paragraph:25349c1228ec_33"},{"__ref":"Paragraph:25349c1228ec_34"},{"__ref":"Paragraph:25349c1228ec_35"},{"__ref":"Paragraph:25349c1228ec_36"},{"__ref":"Paragraph:25349c1228ec_37"},{"__ref":"Paragraph:25349c1228ec_38"},{"__ref":"Paragraph:25349c1228ec_39"},{"__ref":"Paragraph:25349c1228ec_40"},{"__ref":"Paragraph:25349c1228ec_41"},{"__ref":"Paragraph:25349c1228ec_42"},{"__ref":"Paragraph:25349c1228ec_43"},{"__ref":"Paragraph:25349c1228ec_44"},{"__ref":"Paragraph:25349c1228ec_45"},{"__ref":"Paragraph:25349c1228ec_46"},{"__ref":"Paragraph:25349c1228ec_47"},{"__ref":"Paragraph:25349c1228ec_48"},{"__ref":"Paragraph:25349c1228ec_49"},{"__ref":"Paragraph:25349c1228ec_50"},{"__ref":"Paragraph:25349c1228ec_51"},{"__ref":"Paragraph:25349c1228ec_52"},{"__ref":"Paragraph:25349c1228ec_53"},{"__ref":"Paragraph:25349c1228ec_54"},{"__ref":"Paragraph:25349c1228ec_55"},{"__ref":"Paragraph:25349c1228ec_56"},{"__ref":"Paragraph:25349c1228ec_57"},{"__ref":"Paragraph:25349c1228ec_58"},{"__ref":"Paragraph:25349c1228ec_59"},{"__ref":"Paragraph:25349c1228ec_60"},{"__ref":"Paragraph:25349c1228ec_61"},{"__ref":"Paragraph:25349c1228ec_62"},{"__ref":"Paragraph:25349c1228ec_63"},{"__ref":"Paragraph:25349c1228ec_64"},{"__ref":"Paragraph:25349c1228ec_65"},{"__ref":"Paragraph:25349c1228ec_66"},{"__ref":"Paragraph:25349c1228ec_67"},{"__ref":"Paragraph:25349c1228ec_68"},{"__ref":"Paragraph:25349c1228ec_69"},{"__ref":"Paragraph:25349c1228ec_70"},{"__ref":"Paragraph:25349c1228ec_71"},{"__ref":"Paragraph:25349c1228ec_72"},{"__ref":"Paragraph:25349c1228ec_73"},{"__ref":"Paragraph:25349c1228ec_74"},{"__ref":"Paragraph:25349c1228ec_75"},{"__ref":"Paragraph:25349c1228ec_76"},{"__ref":"Paragraph:25349c1228ec_77"},{"__ref":"Paragraph:25349c1228ec_78"},{"__ref":"Paragraph:25349c1228ec_79"},{"__ref":"Paragraph:25349c1228ec_80"},{"__ref":"Paragraph:25349c1228ec_81"},{"__ref":"Paragraph:25349c1228ec_82"},{"__ref":"Paragraph:25349c1228ec_83"},{"__ref":"Paragraph:25349c1228ec_84"},{"__ref":"Paragraph:25349c1228ec_85"},{"__ref":"Paragraph:25349c1228ec_86"},{"__ref":"Paragraph:25349c1228ec_87"},{"__ref":"Paragraph:25349c1228ec_88"},{"__ref":"Paragraph:25349c1228ec_89"},{"__ref":"Paragraph:25349c1228ec_90"}]},"validatedShareKey":"","shareKeyCreator":null},"creator":{"__ref":"User:406aa3cbd38e"},"inResponseToEntityType":null,"isLocked":false,"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Ftransformers-89034557de14","primaryTopic":{"__ref":"Topic:1eca0103fff3"},"topics":[{"__typename":"Topic","slug":"machine-learning"},{"__typename":"Topic","slug":"data-science"}],"isPublished":true,"latestPublishedVersion":"25349c1228ec","visibility":"PUBLIC","postResponses":{"__typename":"PostResponses","count":4},"createdAt":1588854798849,"firstPublishedAt":1593420271505,"latestPublishedAt":1606042906716,"clapCount":624,"allowResponses":true,"isLimitedState":false,"title":"Transformers","isSeries":false,"sequence":null,"uniqueSlug":"transformers-89034557de14","socialTitle":"","socialDek":"","noIndex":null,"canonicalUrl":"","metaDescription":"","readingTime":9.05,"previewContent":{"__typename":"PreviewContent","subtitle":"Or as I like to call it Attention on Steroids. 💉💊"},"previewImage":{"__ref":"ImageMetadata:0*2YfRrpQtt8TG2iuz"},"isShortform":false,"seoTitle":"Transformers in NLP: A beginner friendly explanation","updatedAt":1639533138845,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease using self attention. ","isSuspended":false,"license":"ALL_RIGHTS_RESERVED","tags":[{"__ref":"Tag:artificial-intelligence"},{"__ref":"Tag:machine-learning"},{"__ref":"Tag:nlp"},{"__ref":"Tag:transformers"},{"__ref":"Tag:ai"}],"isNewsletter":false,"statusForCollection":"APPROVED","pendingCollection":null,"detectedLanguage":"en","wordCount":2120,"layerCake":4}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.482ae308.js"></script><script src="https://cdn-client.medium.com/lite/static/js/3057.5e22bbb0.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.29cfb2e5.js"></script><script src="https://cdn-client.medium.com/lite/static/js/instrumentation.5e7f2981.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/reporting.2021fe63.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6068.e9093f2e.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4398.db4d4378.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7883.0e445e04.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9281.e9be8bce.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7111.1421aaa2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6481.e3e8b67f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8695.17d1af21.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6358.a78f5809.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4341.e697d2a1.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5971.fd9e1c6f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5203.e7a22052.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5514.6018be2b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7098.7bbb418a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5700.1c9f05c6.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8491.7fa46461.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8597.31012efa.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1711.b70f1a35.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9174.24f568ee.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8883.c8b03d13.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/705.da9267d6.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5781.39279ff8.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8580.feeb2549.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6046.f9be485b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6605.caa5db7b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/500.80a4127d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9408.9f41b422.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6637.8a411a8c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8565.ce53d42b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4667.993ebc2b.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.12f09e15.chunk.js"></script><script>window.main();</script></body></html>